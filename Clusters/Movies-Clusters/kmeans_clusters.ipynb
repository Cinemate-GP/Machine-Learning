{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9645a8-ba42-41af-a7e3-8cd656b7c1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total genre columns in dataset: 20\n",
      "Top 5 genres in dataset:\n",
      "Genre:Drama: 19750 movies\n",
      "Genre:Comedy: 12380 movies\n",
      "Genre:Thriller: 8275 movies\n",
      "Genre:Action: 6631 movies\n",
      "Genre:Horror: 6335 movies\n",
      "Movies with only one genre: 18774 (41.41%)\n",
      "Movies with only Drama genre: 6667 (14.71%)\n",
      "\n",
      "Genre feature variances:\n",
      "                   Genre  Variance\n",
      "             Genre:Drama  0.000000\n",
      "Genre:(no genres listed)  0.000000\n",
      "            Genre:Comedy  0.000996\n",
      "              Genre:IMAX  0.002022\n",
      "         Genre:Film-Noir  0.002445\n",
      "          Genre:Thriller  0.002597\n",
      "            Genre:Action  0.003420\n",
      "            Genre:Horror  0.003574\n",
      "           Genre:Romance  0.003721\n",
      "           Genre:Musical  0.004012\n",
      "           Genre:Western  0.004267\n",
      "       Genre:Documentary  0.004543\n",
      "             Genre:Crime  0.004575\n",
      "               Genre:War  0.004754\n",
      "         Genre:Adventure  0.004912\n",
      "            Genre:Sci-Fi  0.004920\n",
      "         Genre:Animation  0.005101\n",
      "          Genre:Children  0.005113\n",
      "           Genre:Mystery  0.005118\n",
      "           Genre:Fantasy  0.005120\n",
      "Feature variances (min: 0.0000, max: 0.1985, mean: 0.0504)\n",
      "Warning: Some features have near-zero variance, consider removing them.\n",
      "Running k-means clustering...\n",
      "K-means clustering completed.\n",
      "\n",
      "Top 5 largest clusters:\n",
      "Cluster 99: 1258 movies\n",
      "Cluster 108: 1018 movies\n",
      "Cluster 82: 979 movies\n",
      "Cluster 118: 972 movies\n",
      "Cluster 7: 965 movies\n",
      "Saved largest cluster (Cluster 99) movies to: moooovies_clusters_kmeans/cluster_99_movies.csv\n",
      "\n",
      "Total genre columns in Cluster 99: 20\n",
      "Non-zero genre columns in Cluster 99: 5\n",
      "\n",
      "Top 5 genres in Cluster 99:\n",
      "Genre:Drama: 1253 movies\n",
      "Genre:Sci-Fi: 8 movies\n",
      "Genre:Western: 2 movies\n",
      "Genre:Adventure: 2 movies\n",
      "Genre:Fantasy: 1 movies\n",
      "========== Movie Clustering Analysis Report (K-Means) ==========\n",
      "--- Data Summary ---\n",
      "Total number of movies: 45335\n",
      "Clustering performed on: 64-dimensional latent space from hybrid features\n",
      "--- Clustering Summary ---\n",
      "Number of clusters: 130\n",
      "Movies assigned to clusters: 45335 (100.00%)\n",
      "Average cluster size: 348.7\n",
      "Minimum cluster size: 47\n",
      "Maximum cluster size: 1258\n",
      "Median cluster size: 288.5\n",
      "--- Top 5 Largest Clusters ---\n",
      "Cluster 99: 1258 movies\n",
      "Cluster 108: 1018 movies\n",
      "Cluster 82: 979 movies\n",
      "Cluster 118: 972 movies\n",
      "Cluster 7: 965 movies\n",
      "\n",
      "--- Top 5 Smallest Clusters ---\n",
      "Cluster 129: 47 movies\n",
      "Cluster 127: 78 movies\n",
      "Cluster 54: 94 movies\n",
      "Cluster 65: 96 movies\n",
      "Cluster 81: 98 movies\n",
      "\n",
      "--- Sample Movies from 3 Random Clusters ---\n",
      "\n",
      "Cluster 87 (size: 453):\n",
      " - Watermelon Man\n",
      " - Protocol\n",
      " - Biloxi Blues\n",
      " - We Are All Together\n",
      " - Who's That Girl\n",
      " - Where's the Money\n",
      " - Don Peyote\n",
      "\n",
      "Cluster 46 (size: 786):\n",
      " - The Other Bank\n",
      " - Day of Wrath\n",
      " - Three Days to Forever\n",
      " - Thérèse\n",
      " - Avalon\n",
      " - Sarafina!\n",
      " - The Marquise of O\n",
      "\n",
      "Cluster 20 (size: 263):\n",
      " - Striker\n",
      " - Airplane vs Volcano\n",
      " - Hidden Blade\n",
      " - A Night to Remember\n",
      " - Rolf\n",
      " - Dragon Soldiers\n",
      " - Double Down\n",
      "\n",
      "--- Clustering Quality Metrics ---\n",
      "Silhouette Score (subsample)  : 0.2038\n",
      "Davies–Bouldin Index          : 1.6208\n",
      "Calinski–Harabasz Score       : 278.35\n",
      "==================================================\n",
      "\n",
      "Saved clustering report to: moooovies_clusters_kmeans/clustering_report.txt\n",
      "Saved cluster assignments to: moooovies_clusters_kmeans/movie_clusters_kmeans.csv\n",
      "Saved clustering summary to: moooovies_clusters_kmeans/clustering_summary.json\n",
      "Saved cluster visualization to: moooovies_clusters_kmeans/cluster_visualization.png\n",
      "Saved genre distribution to: moooovies_clusters_kmeans/cluster_genre_distribution.csv\n",
      "Saved genre variances to: moooovies_clusters_kmeans/genre_variances.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.cluster import KMeans\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "CONFIG = {\n",
    "    'data_path':               r\"C:\\Users\\PC\\Downloads\\merged_movies (3).csv\",\n",
    "    'embeddings_path':         \"new_embeddings_distilroberta.npy\",\n",
    "    'output_dir':              \"movies_clusters_kmeans\",\n",
    "    'umap_components':         15,\n",
    "    'umap_n_neighbors':        75,\n",
    "    'umap_min_dist':           0.1,\n",
    "    'umap_metric':             'cosine',\n",
    "    'latent_dim':              64,\n",
    "    'autoencoder_epochs':      50,\n",
    "    'denoising_epochs':        15,\n",
    "    'autoencoder_batch_size':  256,\n",
    "    'denoising_noise_std':     0.05,\n",
    "    'n_clusters':              130,  # Match HDBSCAN's ~130 clusters\n",
    "    'kmeans_random_state':     42,\n",
    "    'genre_weight':            1.5,\n",
    "    'runtime_weight':          0.1,\n",
    "    'year_weight':             0.3,\n",
    "    'sbert_weight':            1.0,\n",
    "    'silhouette_sample_size':   10000\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ─── 1. LOAD & FILTER DATA ─────────────────────────────────────────────────────\n",
    "def load_and_filter_data(config):\n",
    "    \"\"\"Load and preprocess movie dataset.\"\"\"\n",
    "    df = pd.read_csv(config['data_path']).drop_duplicates('MovieID').reset_index(drop=True)\n",
    "    if 'Genre:(no genres listed)' in df.columns:\n",
    "        df = df[df['Genre:(no genres listed)'] == False].reset_index(drop=True)\n",
    "    df['release_year'] = pd.to_numeric(df.get('release_year', np.nan), errors='coerce')\n",
    "    df['release_year'] = df['release_year'].fillna(df['release_year'].median()).clip(1900, 2025)\n",
    "    df['runtime'] = pd.to_numeric(df['runtime'], errors='coerce').fillna(df['runtime'].median())\n",
    "    \n",
    "    # Log genre distribution\n",
    "    genre_cols = [c for c in df.columns if c.startswith('Genre:')]\n",
    "    print(f\"Total genre columns in dataset: {len(genre_cols)}\")\n",
    "    genre_counts = df[genre_cols].sum().sort_values(ascending=False)\n",
    "    print(\"Top 5 genres in dataset:\")\n",
    "    for i in range(min(5, len(genre_counts))):\n",
    "        print(f\"{genre_counts.index[i]}: {int(genre_counts.iloc[i])} movies\")\n",
    "    single_genre_movies = df[genre_cols].sum(axis=1) == 1\n",
    "    drama_only = (df['Genre:Drama'] == 1) & (df[genre_cols].sum(axis=1) == 1)\n",
    "    print(f\"Movies with only one genre: {single_genre_movies.sum()} ({single_genre_movies.sum()/len(df)*100:.2f}%)\")\n",
    "    print(f\"Movies with only Drama genre: {drama_only.sum()} ({drama_only.sum()/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return df, genre_cols\n",
    "\n",
    "df, genre_cols = load_and_filter_data(CONFIG)\n",
    "\n",
    "# ─── 2. LOAD / COMPUTE SBERT EMBEDDINGS ────────────────────────────────────────\n",
    "def compute_sbert_embeddings(df, config):\n",
    "    \"\"\"Load or compute SBERT embeddings for movie overviews.\"\"\"\n",
    "    cache_path = os.path.join(config['output_dir'], 'sbert_embeddings.npy')\n",
    "    if os.path.exists(cache_path):\n",
    "        return np.load(cache_path)\n",
    "    if os.path.exists(config['embeddings_path']):\n",
    "        embeddings = np.load(config['embeddings_path'])\n",
    "    else:\n",
    "        st_model = SentenceTransformer('distilroberta-base', device=device)\n",
    "        embeddings = st_model.encode(\n",
    "            df['overview'].fillna(''),\n",
    "            batch_size=config['autoencoder_batch_size'],\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        np.save(config['embeddings_path'], embeddings)\n",
    "    np.save(cache_path, embeddings)\n",
    "    return embeddings\n",
    "\n",
    "sbert_embeddings = compute_sbert_embeddings(df, CONFIG)\n",
    "\n",
    "# ─── 3. DENOISING AUTOENCODER ────────────────────────────────────────────────────\n",
    "class DenoiseAE(nn.Module):\n",
    "    \"\"\"Denoising Autoencoder for SBERT embeddings.\"\"\"\n",
    "    def __init__(self, inp, lat):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(inp, 512), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(512, lat), nn.ReLU()\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(lat, 512), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(512, inp)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        return self.dec(z), z\n",
    "\n",
    "def train_denoiser(emb, config):\n",
    "    \"\"\"Train denoising autoencoder and return latent representations.\"\"\"\n",
    "    cache_path = os.path.join(config['output_dir'], 'sbert_denoised.npy')\n",
    "    if os.path.exists(cache_path):\n",
    "        return np.load(cache_path)\n",
    "    \n",
    "    model = DenoiseAE(emb.shape[1], config['latent_dim']).to(device)\n",
    "    opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    loader = DataLoader(\n",
    "        TensorDataset(torch.tensor(emb, dtype=torch.float32)),\n",
    "        batch_size=config['autoencoder_batch_size'], shuffle=True\n",
    "    )\n",
    "    best_loss, patience, counter = float('inf'), 3, 0\n",
    "    for epoch in range(config['denoising_epochs']):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for (batch,) in loader:\n",
    "            batch = batch.to(device)\n",
    "            noisy = batch + torch.randn_like(batch) * config['denoising_noise_std']\n",
    "            recon, _ = model(noisy)\n",
    "            loss = nn.MSELoss()(recon, batch)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "        avg = total / len(loader)\n",
    "        if avg < best_loss:\n",
    "            best_loss, counter = avg, 0\n",
    "            torch.save(model.state_dict(), os.path.join(config['output_dir'], 'denoiser.pth'))\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "    model.load_state_dict(torch.load(os.path.join(config['output_dir'], 'denoiser.pth')))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, den = model(torch.tensor(emb, dtype=torch.float32).to(device))\n",
    "    den = den.cpu().numpy()\n",
    "    np.save(cache_path, den)\n",
    "    return den\n",
    "\n",
    "sbert_denoised = train_denoiser(sbert_embeddings, CONFIG)\n",
    "\n",
    "# ─── 4. UMAP REDUCTION ─────────────────────────────────────────────────────────\n",
    "def reduce_umap(embeddings, config):\n",
    "    \"\"\"Apply UMAP reduction to denoised embeddings.\"\"\"\n",
    "    cache_path = os.path.join(config['output_dir'], 'sbert_reduced.npy')\n",
    "    if os.path.exists(cache_path):\n",
    "        return np.load(cache_path)\n",
    "    \n",
    "    um = umap.UMAP(\n",
    "        n_components=config['umap_components'],\n",
    "        n_neighbors=config['umap_n_neighbors'],\n",
    "        min_dist=config['umap_min_dist'],\n",
    "        metric=config['umap_metric'],\n",
    "        random_state=42\n",
    "    )\n",
    "    umap_emb = um.fit_transform(embeddings)\n",
    "    reduced = MinMaxScaler().fit_transform(umap_emb)\n",
    "    np.save(cache_path, reduced)\n",
    "    \n",
    "    # Visualize UMAP projection\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x=reduced[:10000, 0], y=reduced[:10000, 1], s=10, alpha=0.5)\n",
    "    plt.title('UMAP Projection of Denoised SBERT Embeddings (First 10,000 Movies)')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "    plt.savefig(os.path.join(config['output_dir'], 'umap_projection.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return reduced\n",
    "\n",
    "sbert_reduced = reduce_umap(sbert_denoised, CONFIG)\n",
    "\n",
    "# ─── 5. HYBRID FEATURE ENGINEERING ─────────────────────────────────────────────\n",
    "def make_hybrid(df, sbert_red, genre_cols, config):\n",
    "    \"\"\"Create hybrid features from genres, runtime, year, and SBERT embeddings.\"\"\"\n",
    "    # Compute genre weights\n",
    "    w = np.log(len(df) / (df[genre_cols].sum() + 1))\n",
    "    w = MinMaxScaler().fit_transform(w.values.reshape(-1, 1)).flatten()\n",
    "    w[genre_cols.index('Genre:Drama')] *= 0.7\n",
    "    \n",
    "    # Log genre variances\n",
    "    gf = df[genre_cols].values * w * config['genre_weight']\n",
    "    genre_variances = np.var(gf, axis=0)\n",
    "    variance_df = pd.DataFrame({\n",
    "        'Genre': genre_cols,\n",
    "        'Variance': genre_variances\n",
    "    })\n",
    "    print(\"\\nGenre feature variances:\")\n",
    "    print(variance_df.sort_values('Variance').to_string(index=False))\n",
    "    variance_df.to_csv(os.path.join(config['output_dir'], 'genre_variances.csv'), index=False)\n",
    "    \n",
    "    # Scale other features\n",
    "    y = df['release_year'].values.reshape(-1, 1)\n",
    "    yf = MinMaxScaler().fit_transform(y) * config['year_weight']\n",
    "    r = df['runtime'].values.reshape(-1, 1)\n",
    "    rf = MinMaxScaler().fit_transform(r) * config['runtime_weight']\n",
    "    \n",
    "    hybrid = np.hstack([gf, yf, rf, sbert_red * config['sbert_weight']])\n",
    "    hybrid = MinMaxScaler().fit_transform(hybrid)\n",
    "    \n",
    "    # Validate feature variance\n",
    "    feature_variances = np.var(hybrid, axis=0)\n",
    "    print(f\"Feature variances (min: {feature_variances.min():.4f}, max: {feature_variances.max():.4f}, mean: {feature_variances.mean():.4f})\")\n",
    "    if feature_variances.min() < 1e-4:\n",
    "        print(\"Warning: Some features have near-zero variance, consider removing them.\")\n",
    "    \n",
    "    return hybrid, genre_cols\n",
    "\n",
    "hybrid, valid_genre_cols = make_hybrid(df, sbert_reduced, genre_cols, CONFIG)\n",
    "\n",
    "# ─── 6. COMPRESS HYBRID FEATURES ───────────────────────────────────────────────\n",
    "class HybridAE(nn.Module):\n",
    "    \"\"\"Autoencoder for compressing hybrid features.\"\"\"\n",
    "    def __init__(self, inp, lat):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(inp, 256), nn.ReLU(),\n",
    "            nn.Linear(256, lat), nn.ReLU()\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(lat, 256), nn.ReLU(),\n",
    "            nn.Linear(256, inp)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        return self.dec(z), z\n",
    "\n",
    "def train_hybrid(ftrs, config):\n",
    "    \"\"\"Train hybrid autoencoder and return latent representations.\"\"\"\n",
    "    cache_path = os.path.join(config['output_dir'], 'latent_features.npy')\n",
    "    if os.path.exists(cache_path):\n",
    "        return np.load(cache_path)\n",
    "    \n",
    "    model = HybridAE(ftrs.shape[1], config['latent_dim']).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    loader = DataLoader(\n",
    "        TensorDataset(torch.tensor(ftrs, dtype=torch.float32)),\n",
    "        batch_size=config['autoencoder_batch_size'], shuffle=True\n",
    "    )\n",
    "    best, patience, counter = float('inf'), 5, 0\n",
    "    for epoch in range(config['autoencoder_epochs']):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for (batch,) in loader:\n",
    "            batch = batch.to(device)\n",
    "            recon, _ = model(batch)\n",
    "            loss = nn.MSELoss()(recon, batch)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "        avg = total / len(loader)\n",
    "        if avg < best:\n",
    "            best, counter = avg, 0\n",
    "            torch.save(model.state_dict(), os.path.join(config['output_dir'], 'hybrid_ae.pth'))\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "    model.load_state_dict(torch.load(os.path.join(config['output_dir'], 'hybrid_ae.pth')))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, latent = model(torch.tensor(ftrs, dtype=torch.float32).to(device))\n",
    "    latent = latent.cpu().numpy()\n",
    "    np.save(cache_path, latent)\n",
    "    return latent\n",
    "\n",
    "latent = train_hybrid(hybrid, CONFIG)\n",
    "\n",
    "# ─── 7. K-MEANS CLUSTERING ────────────────────────────────────────────────────\n",
    "def cluster_kmeans(latent, config):\n",
    "    \"\"\"Apply k-means clustering to latent features.\"\"\"\n",
    "    print(\"Running k-means clustering...\")\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=config['n_clusters'],\n",
    "        random_state=config['kmeans_random_state'],\n",
    "        n_init=10,\n",
    "        max_iter=300\n",
    "    )\n",
    "    labels = kmeans.fit_predict(latent)\n",
    "    print(\"K-means clustering completed.\")\n",
    "    return labels\n",
    "\n",
    "labels = cluster_kmeans(latent, CONFIG)\n",
    "\n",
    "# ─── 8. CLUSTER SIZE REPORTING ─────────────────────────────────────────────────\n",
    "def report_clusters(df, labels, genre_cols, config):\n",
    "    \"\"\"Generate and save cluster statistics and visualizations.\"\"\"\n",
    "    cluster_sizes = pd.Series(labels).value_counts().sort_values(ascending=False)\n",
    "    print(\"\\nTop 5 largest clusters:\")\n",
    "    for i in range(5):\n",
    "        if i < len(cluster_sizes):\n",
    "            print(f\"Cluster {cluster_sizes.index[i]}: {cluster_sizes.iloc[i]} movies\")\n",
    "    \n",
    "    # Save largest cluster\n",
    "    if len(cluster_sizes) > 0:\n",
    "        largest_cluster_id = cluster_sizes.index[0]\n",
    "        largest_cluster_movies = df[labels == largest_cluster_id][['MovieID', 'title', 'release_year', 'runtime'] + genre_cols]\n",
    "        largest_cluster_movies.to_csv(os.path.join(config['output_dir'], f'cluster_{largest_cluster_id}_movies.csv'), index=False)\n",
    "        print(f\"Saved largest cluster (Cluster {largest_cluster_id}) movies to: {config['output_dir']}/cluster_{largest_cluster_id}_movies.csv\")\n",
    "        genre_counts = largest_cluster_movies[genre_cols].sum().sort_values(ascending=False)\n",
    "        print(f\"\\nTotal genre columns in Cluster {largest_cluster_id}: {len(genre_cols)}\")\n",
    "        print(f\"Non-zero genre columns in Cluster {largest_cluster_id}: {sum(genre_counts > 0)}\")\n",
    "        print(f\"\\nTop 5 genres in Cluster {largest_cluster_id}:\")\n",
    "        for i in range(min(5, len(genre_counts))):\n",
    "            print(f\"{genre_counts.index[i]}: {int(genre_counts.iloc[i])} movies\")\n",
    "    \n",
    "    return cluster_sizes\n",
    "\n",
    "cluster_sizes = report_clusters(df, labels, genre_cols, CONFIG)\n",
    "\n",
    "# ─── 9. FINAL METRICS & VISUALIZATION ──────────────────────────────────────────\n",
    "def compute_metrics_and_visualize(df, latent, labels, cluster_sizes, config):\n",
    "    \"\"\"Compute clustering metrics and generate visualizations.\"\"\"\n",
    "    # Compute metrics\n",
    "    np.random.seed(42)\n",
    "    idx = np.random.choice(len(latent), min(len(latent), config['silhouette_sample_size']), replace=False)\n",
    "    sil_score, db_index, ch_score = 0.0, np.inf, 0.0\n",
    "    if len(np.unique(labels[idx])) > 1:\n",
    "        sil_score = silhouette_score(latent[idx], labels[idx])\n",
    "        db_index = davies_bouldin_score(latent[idx], labels[idx])\n",
    "        ch_score = calinski_harabasz_score(latent[idx], labels[idx])\n",
    "    \n",
    "    # Cluster statistics\n",
    "    num_clusters = len(cluster_sizes)\n",
    "    total_movies = len(labels)\n",
    "    movies_in_clusters = total_movies\n",
    "    percentage_in_clusters = 100.0\n",
    "    avg_size = cluster_sizes.mean() if num_clusters > 0 else 0\n",
    "    min_size = cluster_sizes.min() if num_clusters > 0 else 0\n",
    "    max_size = cluster_sizes.max() if num_clusters > 0 else 0\n",
    "    median_size = cluster_sizes.median() if num_clusters > 0 else 0\n",
    "    \n",
    "    # Generate report\n",
    "    report_lines = []\n",
    "    report_lines.append(\"========== Movie Clustering Analysis Report (K-Means) ==========\\n\")\n",
    "    report_lines.append(\"--- Data Summary ---\\n\")\n",
    "    report_lines.append(f\"Total number of movies: {total_movies}\\n\")\n",
    "    report_lines.append(f\"Clustering performed on: {config['latent_dim']}-dimensional latent space from hybrid features\\n\")\n",
    "    report_lines.append(\"--- Clustering Summary ---\\n\")\n",
    "    report_lines.append(f\"Number of clusters: {num_clusters}\\n\")\n",
    "    report_lines.append(f\"Movies assigned to clusters: {movies_in_clusters} ({percentage_in_clusters:.2f}%)\\n\")\n",
    "    report_lines.append(f\"Average cluster size: {avg_size:.1f}\\n\")\n",
    "    report_lines.append(f\"Minimum cluster size: {min_size}\\n\")\n",
    "    report_lines.append(f\"Maximum cluster size: {max_size}\\n\")\n",
    "    report_lines.append(f\"Median cluster size: {median_size}\\n\")\n",
    "    report_lines.append(\"--- Top 5 Largest Clusters ---\\n\")\n",
    "    for i, (cluster, size) in enumerate(cluster_sizes.items()):\n",
    "        if i < 5:\n",
    "            report_lines.append(f\"Cluster {cluster}: {size} movies\\n\")\n",
    "    report_lines.append(\"\\n--- Top 5 Smallest Clusters ---\\n\")\n",
    "    for i, (cluster, size) in enumerate(cluster_sizes.sort_values().items()):\n",
    "        if i < 5:\n",
    "            report_lines.append(f\"Cluster {cluster}: {size} movies\\n\")\n",
    "    \n",
    "    # Sample movies\n",
    "    random.seed(42)\n",
    "    sample_clusters = random.sample(list(cluster_sizes.index), min(3, num_clusters))\n",
    "    report_lines.append(\"\\n--- Sample Movies from 3 Random Clusters ---\\n\")\n",
    "    for cluster in sample_clusters:\n",
    "        cluster_movies = df[labels == cluster]['title'].tolist()\n",
    "        sample_titles = random.sample(cluster_movies, min(7, len(cluster_movies)))\n",
    "        report_lines.append(f\"\\nCluster {cluster} (size: {cluster_sizes[cluster]}):\\n\")\n",
    "        for title in sample_titles:\n",
    "            report_lines.append(f\" - {title}\\n\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    report_lines.append(\"\\n--- Clustering Quality Metrics ---\\n\")\n",
    "    report_lines.append(f\"{'Silhouette Score (subsample)':<30}: {sil_score:.4f}\\n\")\n",
    "    report_lines.append(f\"{'Davies–Bouldin Index':<30}: {db_index:.4f}\\n\")\n",
    "    report_lines.append(f\"{'Calinski–Harabasz Score':<30}: {ch_score:.2f}\\n\")\n",
    "    report_lines.append(\"==================================================\\n\")\n",
    "    \n",
    "    # Print and save report\n",
    "    with open(os.path.join(config['output_dir'], 'clustering_report.txt'), 'w') as f:\n",
    "        for line in report_lines:\n",
    "            print(line, end='')\n",
    "            f.write(line)\n",
    "    \n",
    "    # Save cluster assignments\n",
    "    pd.DataFrame({'MovieID': df['MovieID'], 'cluster': labels}).to_csv(\n",
    "        os.path.join(config['output_dir'], 'movie_clusters_kmeans.csv'), index=False\n",
    "    )\n",
    "    \n",
    "    # Save JSON summary\n",
    "    summary = {\n",
    "        'total_movies': int(total_movies),\n",
    "        'num_clusters': int(num_clusters),\n",
    "        'movies_in_clusters': int(movies_in_clusters),\n",
    "        'percentage_in_clusters': float(percentage_in_clusters),\n",
    "        'avg_cluster_size': float(avg_size),\n",
    "        'min_cluster_size': int(min_size),\n",
    "        'max_cluster_size': int(max_size),\n",
    "        'median_cluster_size': float(median_size),\n",
    "        'silhouette_score': float(sil_score),\n",
    "        'davies_bouldin_index': float(db_index),\n",
    "        'calinski_harabasz_score': float(ch_score)\n",
    "    }\n",
    "    with open(os.path.join(config['output_dir'], 'clustering_summary.json'), 'w') as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "    \n",
    "    # Visualize clusters in UMAP space\n",
    "    umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    umap_2d = umap_reducer.fit_transform(latent)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(\n",
    "        umap_2d[:, 0], umap_2d[:, 1],\n",
    "        c=labels, cmap='Spectral', s=5, alpha=0.6\n",
    "    )\n",
    "    plt.colorbar(scatter, label='Cluster ID')\n",
    "    plt.title('UMAP Visualization of Movie Clusters (K-Means)')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "    plt.savefig(os.path.join(config['output_dir'], 'cluster_visualization.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Generate cluster genre distribution table\n",
    "    cluster_genre_dist = []\n",
    "    for cluster in cluster_sizes.index:\n",
    "        cluster_movies = df[labels == cluster]\n",
    "        genre_counts = cluster_movies[genre_cols].sum().to_dict()\n",
    "        genre_counts['Cluster'] = int(cluster)\n",
    "        genre_counts['Size'] = int(cluster_sizes[cluster])\n",
    "        cluster_genre_dist.append(genre_counts)\n",
    "    pd.DataFrame(cluster_genre_dist).to_csv(\n",
    "        os.path.join(config['output_dir'], 'cluster_genre_distribution.csv'), index=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSaved clustering report to: {config['output_dir']}/clustering_report.txt\")\n",
    "    print(f\"Saved cluster assignments to: {config['output_dir']}/movie_clusters_kmeans.csv\")\n",
    "    print(f\"Saved clustering summary to: {config['output_dir']}/clustering_summary.json\")\n",
    "    print(f\"Saved cluster visualization to: {config['output_dir']}/cluster_visualization.png\")\n",
    "    print(f\"Saved genre distribution to: {config['output_dir']}/cluster_genre_distribution.csv\")\n",
    "    print(f\"Saved genre variances to: {config['output_dir']}/genre_variances.csv\")\n",
    "\n",
    "compute_metrics_and_visualize(df, latent, labels, cluster_sizes, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2da2d8-1953-47cc-8e77-506685f76f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
