{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e29485d3-b9b5-4342-bd42-d892b782300d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total genre columns in dataset: 20\n",
      "Top 5 genres in dataset:\n",
      "Genre:Drama: 19750 movies\n",
      "Genre:Comedy: 12380 movies\n",
      "Genre:Thriller: 8275 movies\n",
      "Genre:Action: 6631 movies\n",
      "Genre:Horror: 6335 movies\n",
      "Movies with only one genre: 18774 (41.41%)\n",
      "Movies with only Drama genre: 6667 (14.71%)\n",
      "Feature variances (min: 0.0000, max: 0.1985, mean: 0.0512)\n",
      "Warning: Some features have near-zero variance, consider removing them.\n",
      "Noise after HDBSCAN: 6.95%\n",
      "Noise points before reassignment: 3151 (6.95%)\n",
      "Reassigned 2836 noise points\n",
      "Noise after reassignment: 0.69%\n",
      "\n",
      "Top 5 largest clusters:\n",
      "Cluster 104: 6667 movies\n",
      "Cluster 103: 5146 movies\n",
      "Cluster 101: 3965 movies\n",
      "Cluster 88: 2065 movies\n",
      "Cluster 65: 2007 movies\n",
      "Saved largest cluster (Cluster 104) movies to: final_movies_cluster1/cluster_104_movies.csv\n",
      "\n",
      "Total genre columns in Cluster 104: 20\n",
      "Non-zero genre columns in Cluster 104: 1\n",
      "\n",
      "Top 5 genres in Cluster 104:\n",
      "Genre:Drama: 6667 movies\n",
      "Genre:IMAX: 0 movies\n",
      "Genre:Thriller: 0 movies\n",
      "Genre:Musical: 0 movies\n",
      "Genre:Comedy: 0 movies\n",
      "========== Movie Clustering Analysis Report ==========\n",
      "--- Data Summary ---\n",
      "Total number of movies: 45335\n",
      "Clustering performed on: 64-dimensional latent space from hybrid features\n",
      "--- Clustering Summary ---\n",
      "Number of clusters: 109\n",
      "Movies assigned to clusters: 45020 (99.31%)\n",
      "Average cluster size: 413.0\n",
      "Minimum cluster size: 60\n",
      "Maximum cluster size: 6667\n",
      "Median cluster size: 148.0\n",
      "--- Top 5 Largest Clusters ---\n",
      "Cluster 104: 6667 movies\n",
      "Cluster 103: 5146 movies\n",
      "Cluster 101: 3965 movies\n",
      "Cluster 88: 2065 movies\n",
      "Cluster 65: 2007 movies\n",
      "\n",
      "--- Top 5 Smallest Clusters ---\n",
      "Cluster 4: 60 movies\n",
      "Cluster 75: 64 movies\n",
      "Cluster 83: 65 movies\n",
      "Cluster 92: 65 movies\n",
      "Cluster 80: 67 movies\n",
      "\n",
      "--- Sample Movies from 3 Random Clusters ---\n",
      "\n",
      "Cluster 28 (size: 98):\n",
      " - Asteroid City\n",
      " - Frequencies\n",
      " - Perfect Sense\n",
      " - The Little Girl Who Conquered Time\n",
      " - The Invisible Woman\n",
      " - It's All About Love\n",
      " - Zanox\n",
      "\n",
      "Cluster 66 (size: 477):\n",
      " - Beneath the Surface\n",
      " - ManFish\n",
      " - Hard Rock Zombies\n",
      " - Ed and His Dead Mother\n",
      " - Elvira's Haunted Hills\n",
      " - Hello Ghost\n",
      " - The Toxic Avenger Part II\n",
      "\n",
      "Cluster 88 (size: 2065):\n",
      " - The Butcher's Wife\n",
      " - The Cooler\n",
      " - Love Before Breakfast\n",
      " - Amira & Sam\n",
      " - Billy's Hollywood Screen Kiss\n",
      " - The Giant Mechanical Man\n",
      " - Fuck My Family\n",
      "\n",
      "--- Clustering Quality Metrics ---\n",
      "Silhouette Score (subsample)  : 0.2678\n",
      "Davies–Bouldin Index          : 1.4483\n",
      "Calinski–Harabasz Score       : 224.75\n",
      "==================================================\n",
      "\n",
      "Saved clustering report to: final_movies_cluster1/clustering_report.txt\n",
      "Saved cluster assignments to: final_movies_cluster1/movie_clusters_optimized.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import umap\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "CONFIG = {\n",
    "    'data_path':               r\"C:\\Users\\PC\\Downloads\\merged_movies (3).csv\",\n",
    "    'embeddings_path':         \"new_embeddings_distilroberta.npy\",\n",
    "    'output_dir':              \"final_movies_clusters\",\n",
    "    'umap_components':         15,\n",
    "    'umap_n_neighbors':        75,\n",
    "    'umap_min_dist':           0.1,\n",
    "    'umap_metric':             'cosine',\n",
    "    'latent_dim':              64,\n",
    "    'autoencoder_epochs':      50,\n",
    "    'denoising_epochs':        15,\n",
    "    'autoencoder_batch_size':  256,\n",
    "    'denoising_noise_std':     0.05,\n",
    "    # Original HDBSCAN parameters\n",
    "    'hdbscan_min_cluster_size': 50,\n",
    "    'hdbscan_min_samples':      5,\n",
    "    'hdbscan_epsilon':          0.3,\n",
    "    'hdbscan_method':           'eom',\n",
    "    # Feature weights \n",
    "    'genre_weight':            1.5,\n",
    "    'runtime_weight':          0.1,\n",
    "    'year_weight':             0.3,\n",
    "    'sbert_weight':            1.0,\n",
    "    # Subsample for metrics\n",
    "    'silhouette_sample_size':   10000,\n",
    "    # Stricter noise reassignment threshold\n",
    "    'noise_reassign_threshold': 90\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ─── 1. LOAD & FILTER DATA ─────────────────────────────────────────────────────\n",
    "df = pd.read_csv(CONFIG['data_path']).drop_duplicates('MovieID').reset_index(drop=True)\n",
    "if 'Genre:(no genres listed)' in df.columns:\n",
    "    df = df[df['Genre:(no genres listed)'] == False].reset_index(drop=True)\n",
    "df['release_year'] = pd.to_numeric(df.get('release_year', np.nan), errors='coerce')\n",
    "df['release_year'] = df['release_year'].fillna(df['release_year'].median()).clip(1900, 2025)\n",
    "df['runtime'] = pd.to_numeric(df['runtime'], errors='coerce').fillna(df['runtime'].median())\n",
    "\n",
    "# Debug dataset-wide genre distribution\n",
    "genre_cols = [c for c in df.columns if c.startswith('Genre:')]\n",
    "print(f\"Total genre columns in dataset: {len(genre_cols)}\")\n",
    "genre_counts = df[genre_cols].sum().sort_values(ascending=False)\n",
    "print(f\"Top 5 genres in dataset:\")\n",
    "for i in range(min(5, len(genre_counts))):\n",
    "    print(f\"{genre_counts.index[i]}: {int(genre_counts.iloc[i])} movies\")\n",
    "single_genre_movies = df[genre_cols].sum(axis=1) == 1\n",
    "drama_only = (df['Genre:Drama'] == 1) & (df[genre_cols].sum(axis=1) == 1)\n",
    "print(f\"Movies with only one genre: {single_genre_movies.sum()} ({single_genre_movies.sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"Movies with only Drama genre: {drama_only.sum()} ({drama_only.sum()/len(df)*100:.2f}%)\")\n",
    "\n",
    "# ─── 2. LOAD / COMPUTE SBERT EMBEDDINGS ────────────────────────────────────────\n",
    "if os.path.exists(CONFIG['embeddings_path']):\n",
    "    sbert_embeddings = np.load(CONFIG['embeddings_path'])\n",
    "else:\n",
    "    st_model = SentenceTransformer('distilroberta-base', device=device)\n",
    "    sbert_embeddings = st_model.encode(\n",
    "        df['overview'].fillna(''),\n",
    "        batch_size=CONFIG['autoencoder_batch_size'],\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    np.save(CONFIG['embeddings_path'], sbert_embeddings)\n",
    "\n",
    "# ─── 3. DENOISING AUTOENCODER ────────────────────────────────────────────────────\n",
    "class DenoiseAE(nn.Module):\n",
    "    def __init__(self, inp, lat):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(inp, 512), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(512, lat), nn.ReLU()\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(lat, 512), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(512, inp)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        return self.dec(z), z\n",
    "\n",
    "def train_denoiser(emb):\n",
    "    model = DenoiseAE(emb.shape[1], CONFIG['latent_dim']).to(device)\n",
    "    opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    loader = DataLoader(\n",
    "        TensorDataset(torch.tensor(emb, dtype=torch.float32)),\n",
    "        batch_size=CONFIG['autoencoder_batch_size'], shuffle=True\n",
    "    )\n",
    "    best_loss, patience, counter = float('inf'), 3, 0\n",
    "    for epoch in range(CONFIG['denoising_epochs']):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for (batch,) in loader:\n",
    "            batch = batch.to(device)\n",
    "            noisy = batch + torch.randn_like(batch) * CONFIG['denoising_noise_std']\n",
    "            recon, _ = model(noisy)\n",
    "            loss = nn.MSELoss()(recon, batch)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "        avg = total / len(loader)\n",
    "        if avg < best_loss:\n",
    "            best_loss, counter = avg, 0\n",
    "            torch.save(model.state_dict(), os.path.join(CONFIG['output_dir'], 'denoiser.pth'))\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "    model.load_state_dict(torch.load(os.path.join(CONFIG['output_dir'], 'denoiser.pth')))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, den = model(torch.tensor(emb, dtype=torch.float32).to(device))\n",
    "    return den.cpu().numpy()\n",
    "\n",
    "sbert_denoised = train_denoiser(sbert_embeddings)\n",
    "\n",
    "# ─── 4. UMAP REDUCTION ─────────────────────────────────────────────────────────\n",
    "um = umap.UMAP(\n",
    "    n_components=CONFIG['umap_components'],\n",
    "    n_neighbors=CONFIG['umap_n_neighbors'],\n",
    "    min_dist=CONFIG['umap_min_dist'],\n",
    "    metric=CONFIG['umap_metric'],\n",
    "    random_state=42\n",
    ")\n",
    "umap_emb = um.fit_transform(sbert_denoised)\n",
    "sbert_reduced = MinMaxScaler().fit_transform(umap_emb)\n",
    "\n",
    "# Visualize UMAP projection for debugging\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=sbert_reduced[:10000, 0], y=sbert_reduced[:10000, 1], s=10, alpha=0.5)\n",
    "plt.title('UMAP Projection of Denoised SBERT Embeddings (First 10,000 Movies)')\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'umap_projection.png'))\n",
    "plt.close()\n",
    "\n",
    "# ─── 5. HYBRID FEATURE ENGINEERING ─────────────────────────────────────────────\n",
    "def make_hybrid(df, sbert_red):\n",
    "    gcols = [c for c in df.columns if c.startswith('Genre:')]\n",
    "    # Normalize genre weights and reduce Drama dominance\n",
    "    w = np.log(len(df) / (df[gcols].sum() + 1))\n",
    "    w = MinMaxScaler().fit_transform(w.values.reshape(-1, 1)).flatten()  # Convert Series to array and normalize\n",
    "    w[gcols.index('Genre:Drama')] *= 0.7  # Stronger reduction\n",
    "    gf = df[gcols].values * w * CONFIG['genre_weight']\n",
    "    y = df['release_year'].values.reshape(-1, 1)\n",
    "    yf = MinMaxScaler().fit_transform(y) * CONFIG['year_weight']\n",
    "    r = df['runtime'].values.reshape(-1, 1)\n",
    "    rf = MinMaxScaler().fit_transform(r) * CONFIG['runtime_weight']\n",
    "    return np.hstack([gf, yf, rf, sbert_red * CONFIG['sbert_weight']])\n",
    "\n",
    "hybrid = make_hybrid(df, sbert_reduced)\n",
    "hybrid = MinMaxScaler().fit_transform(hybrid)\n",
    "\n",
    "# Validate feature variance\n",
    "feature_variances = np.var(hybrid, axis=0)\n",
    "print(f\"Feature variances (min: {feature_variances.min():.4f}, max: {feature_variances.max():.4f}, mean: {feature_variances.mean():.4f})\")\n",
    "if feature_variances.min() < 1e-4:\n",
    "    print(\"Warning: Some features have near-zero variance, consider removing them.\")\n",
    "\n",
    "# ─── 6. COMPRESS HYBRID FEATURES ───────────────────────────────────────────────\n",
    "class HybridAE(nn.Module):\n",
    "    def __init__(self, inp, lat):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(inp, 256), nn.ReLU(),\n",
    "            nn.Linear(256, lat), nn.ReLU()\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(lat, 256), nn.ReLU(),\n",
    "            nn.Linear(256, inp)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        return self.dec(z), z\n",
    "\n",
    "def train_hybrid(ftrs):\n",
    "    model = HybridAE(ftrs.shape[1], CONFIG['latent_dim']).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    loader = DataLoader(\n",
    "        TensorDataset(torch.tensor(ftrs, dtype=torch.float32)),\n",
    "        batch_size=CONFIG['autoencoder_batch_size'], shuffle=True\n",
    "    )\n",
    "    best, patience, counter = float('inf'), 5, 0\n",
    "    for epoch in range(CONFIG['autoencoder_epochs']):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for (batch,) in loader:\n",
    "            batch = batch.to(device)\n",
    "            recon, _ = model(batch)\n",
    "            loss = nn.MSELoss()(recon, batch)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "        avg = total / len(loader)\n",
    "        if avg < best:\n",
    "            best, counter = avg, 0\n",
    "            torch.save(model.state_dict(), os.path.join(CONFIG['output_dir'], 'hybrid_ae.pth'))\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "    model.load_state_dict(torch.load(os.path.join(CONFIG['output_dir'], 'hybrid_ae.pth')))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, latent = model(torch.tensor(ftrs, dtype=torch.float32).to(device))\n",
    "    return latent.cpu().numpy()\n",
    "\n",
    "latent = train_hybrid(hybrid)\n",
    "\n",
    "# ─── 7. HDBSCAN CLUSTERING ─────────────────────────────────────────────────────\n",
    "ds_hdb = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=CONFIG['hdbscan_min_cluster_size'],\n",
    "    min_samples=CONFIG['hdbscan_min_samples'],\n",
    "    cluster_selection_epsilon=CONFIG['hdbscan_epsilon'],\n",
    "    cluster_selection_method=CONFIG['hdbscan_method'],\n",
    "    metric='euclidean',\n",
    "    core_dist_n_jobs=-1\n",
    ")\n",
    "labels = ds_hdb.fit_predict(latent)\n",
    "print(f\"Noise after HDBSCAN: {np.mean(labels == -1)*100:.2f}%\")\n",
    "\n",
    "# ─── 8. SELECTIVE NOISE REASSIGNMENT ───────────────────────────────────────────\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "mask_noise = labels == -1\n",
    "noise_count = np.sum(mask_noise)\n",
    "print(f\"Noise points before reassignment: {noise_count} ({noise_count/len(labels)*100:.2f}%)\")\n",
    "if mask_noise.any():\n",
    "    valid_labels = np.unique(labels[~mask_noise])\n",
    "    centroids = np.vstack([latent[labels == c].mean(axis=0) for c in valid_labels])\n",
    "    distances = euclidean_distances(latent[mask_noise], centroids)\n",
    "    min_distances = distances.min(axis=1)\n",
    "    threshold = np.percentile(min_distances, CONFIG['noise_reassign_threshold'])\n",
    "    noise_indices = np.where(mask_noise)[0]\n",
    "    reassign_count = 0\n",
    "    for i, dist in enumerate(min_distances):\n",
    "        if dist <= threshold:\n",
    "            closest_cluster = valid_labels[np.argmin(distances[i])]\n",
    "            labels[noise_indices[i]] = closest_cluster\n",
    "            reassign_count += 1\n",
    "    print(f\"Reassigned {reassign_count} noise points\")\n",
    "else:\n",
    "    print(\"No noise points to reassign\")\n",
    "\n",
    "final_noise_count = np.sum(labels == -1)\n",
    "print(f\"Noise after reassignment: {final_noise_count/len(labels)*100:.2f}%\")\n",
    "\n",
    "# ─── 9. CLUSTER SIZE REPORTING ─────────────────────────────────────────────────\n",
    "cluster_sizes = pd.Series(labels[labels != -1]).value_counts().sort_values(ascending=False)\n",
    "print(\"\\nTop 5 largest clusters:\")\n",
    "for i in range(5):\n",
    "    if i < len(cluster_sizes):\n",
    "        print(f\"Cluster {cluster_sizes.index[i]}: {cluster_sizes.iloc[i]} movies\")\n",
    "\n",
    "# Save largest cluster's movies to CSV\n",
    "if len(cluster_sizes) > 0:\n",
    "    largest_cluster_id = cluster_sizes.index[0]\n",
    "    largest_cluster_movies = df[labels == largest_cluster_id][['MovieID', 'title', 'release_year', 'runtime'] + genre_cols]\n",
    "    largest_cluster_movies.to_csv(os.path.join(CONFIG['output_dir'], f'cluster_{largest_cluster_id}_movies.csv'), index=False)\n",
    "    print(f\"Saved largest cluster (Cluster {largest_cluster_id}) movies to: {CONFIG['output_dir']}/cluster_{largest_cluster_id}_movies.csv\")\n",
    "    genre_counts = largest_cluster_movies[genre_cols].sum().sort_values(ascending=False)\n",
    "    print(f\"\\nTotal genre columns in Cluster {largest_cluster_id}: {len(genre_cols)}\")\n",
    "    print(f\"Non-zero genre columns in Cluster {largest_cluster_id}: {sum(genre_counts > 0)}\")\n",
    "    print(f\"\\nTop 5 genres in Cluster {largest_cluster_id}:\")\n",
    "    for i in range(min(5, len(genre_counts))):\n",
    "        print(f\"{genre_counts.index[i]}: {int(genre_counts.iloc[i])} movies\")\n",
    "\n",
    "# ─── 10. FINAL METRICS & SAVE ──────────────────────────────────────────────────\n",
    "# Compute final metrics\n",
    "np.random.seed(42)\n",
    "idx = np.random.choice(len(latent), min(len(latent), CONFIG['silhouette_sample_size']), replace=False)\n",
    "mask = labels[idx] != -1\n",
    "if mask.any():\n",
    "    sil_score = silhouette_score(latent[idx][mask], labels[idx][mask])\n",
    "    db_index = davies_bouldin_score(latent[idx][mask], labels[idx][mask])\n",
    "    ch_score = calinski_harabasz_score(latent[idx][mask], labels[idx][mask])\n",
    "else:\n",
    "    sil_score, db_index, ch_score = 0.0, np.inf, 0.0\n",
    "\n",
    "# Compute cluster statistics\n",
    "num_clusters = len(cluster_sizes)\n",
    "total_movies = len(labels)\n",
    "movies_in_clusters = total_movies - final_noise_count\n",
    "percentage_in_clusters = (movies_in_clusters / total_movies) * 100\n",
    "avg_size = cluster_sizes.mean() if num_clusters > 0 else 0\n",
    "min_size = cluster_sizes.min() if num_clusters > 0 else 0\n",
    "max_size = cluster_sizes.max() if num_clusters > 0 else 0\n",
    "median_size = cluster_sizes.median() if num_clusters > 0 else 0\n",
    "\n",
    "# Generate report\n",
    "report_lines = []\n",
    "report_lines.append(\"========== Movie Clustering Analysis Report ==========\\n\")\n",
    "report_lines.append(\"--- Data Summary ---\\n\")\n",
    "report_lines.append(f\"Total number of movies: {total_movies}\\n\")\n",
    "report_lines.append(f\"Clustering performed on: {CONFIG['latent_dim']}-dimensional latent space from hybrid features\\n\")\n",
    "report_lines.append(\"--- Clustering Summary ---\\n\")\n",
    "report_lines.append(f\"Number of clusters: {num_clusters}\\n\")\n",
    "report_lines.append(f\"Movies assigned to clusters: {movies_in_clusters} ({percentage_in_clusters:.2f}%)\\n\")\n",
    "report_lines.append(f\"Average cluster size: {avg_size:.1f}\\n\")\n",
    "report_lines.append(f\"Minimum cluster size: {min_size}\\n\")\n",
    "report_lines.append(f\"Maximum cluster size: {max_size}\\n\")\n",
    "report_lines.append(f\"Median cluster size: {median_size}\\n\")\n",
    "report_lines.append(\"--- Top 5 Largest Clusters ---\\n\")\n",
    "for i, (cluster, size) in enumerate(cluster_sizes.items()):\n",
    "    if i < 5:\n",
    "        report_lines.append(f\"Cluster {cluster}: {size} movies\\n\")\n",
    "report_lines.append(\"\\n--- Top 5 Smallest Clusters ---\\n\")\n",
    "for i, (cluster, size) in enumerate(cluster_sizes.sort_values().items()):\n",
    "    if i < 5:\n",
    "        report_lines.append(f\"Cluster {cluster}: {size} movies\\n\")\n",
    "\n",
    "# Sample movies\n",
    "random.seed(42)\n",
    "sample_clusters = random.sample(list(cluster_sizes.index), min(3, num_clusters))\n",
    "report_lines.append(\"\\n--- Sample Movies from 3 Random Clusters ---\\n\")\n",
    "for cluster in sample_clusters:\n",
    "    cluster_movies = df[labels == cluster]['title'].tolist()\n",
    "    sample_titles = random.sample(cluster_movies, min(7, len(cluster_movies)))\n",
    "    report_lines.append(f\"\\nCluster {cluster} (size: {cluster_sizes[cluster]}):\\n\")\n",
    "    for title in sample_titles:\n",
    "        report_lines.append(f\" - {title}\\n\")\n",
    "\n",
    "# Quality metrics\n",
    "report_lines.append(\"\\n--- Clustering Quality Metrics ---\\n\")\n",
    "report_lines.append(f\"{'Silhouette Score (subsample)':<30}: {sil_score:.4f}\\n\")\n",
    "report_lines.append(f\"{'Davies–Bouldin Index':<30}: {db_index:.4f}\\n\")\n",
    "report_lines.append(f\"{'Calinski–Harabasz Score':<30}: {ch_score:.2f}\\n\")\n",
    "report_lines.append(\"==================================================\\n\")\n",
    "\n",
    "# Print and save report\n",
    "with open(os.path.join(CONFIG['output_dir'], 'clustering_report.txt'), 'w') as f:\n",
    "    for line in report_lines:\n",
    "        print(line, end='')\n",
    "        f.write(line)\n",
    "\n",
    "# Save clusters to CSV\n",
    "pd.DataFrame({'MovieID': df['MovieID'], 'cluster': labels}).to_csv(\n",
    "    os.path.join(CONFIG['output_dir'], 'movie_clusters_optimized.csv'), index=False\n",
    ")\n",
    "print(f\"\\nSaved clustering report to: {CONFIG['output_dir']}/clustering_report.txt\")\n",
    "print(f\"Saved cluster assignments to: {CONFIG['output_dir']}/movie_clusters_optimized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2416b99e-bc50-4a24-bd88-0970c3e8be6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2D UMAP visualization of clusters with more distinct colors...\n",
      "Saved 2D cluster visualization with more distinct colors to: final_movies_cluster1/cluster_visualization_2d_more_distinct.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import umap\n",
    "import os\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "# Assume 'latent', 'labels', and 'CONFIG' are already defined from previous clustering steps\n",
    "# Visualize clusters in 2D using UMAP with more distinct colors\n",
    "print(\"Generating 2D UMAP visualization of clusters with more distinct colors...\")\n",
    "umap_2d = umap.UMAP(n_components=2, random_state=42)\n",
    "latent_2d = umap_2d.fit_transform(latent)\n",
    "\n",
    "# Get unique cluster labels, excluding noise\n",
    "unique_labels = np.unique(labels[labels != -1])\n",
    "\n",
    "# Combine multiple colormaps for more distinct colors\n",
    "base_cmaps = ['tab20', 'tab20b', 'tab20c']  # Each has 20 colors, totaling 60 unique colors\n",
    "base_colors = np.vstack([plt.get_cmap(cmap)(np.linspace(0, 1, 20)) for cmap in base_cmaps])\n",
    "\n",
    "# Repeat base colors if needed to cover all clusters\n",
    "num_repeats = int(np.ceil(len(unique_labels) / len(base_colors)))\n",
    "colors = np.tile(base_colors, (num_repeats, 1))[:len(unique_labels)]\n",
    "\n",
    "# Create a custom colormap for clusters\n",
    "cluster_cmap = ListedColormap(colors)\n",
    "\n",
    "# Convert noise color to RGBA for consistency\n",
    "noise_color = to_rgba('gray')  # Shape (4,) for RGBA\n",
    "\n",
    "# Create a color array for all points\n",
    "plot_colors = np.zeros((len(labels), 4))  # Initialize array for RGBA colors\n",
    "for i, label in enumerate(labels):\n",
    "    if label == -1:\n",
    "        plot_colors[i] = noise_color\n",
    "    else:\n",
    "        plot_colors[i] = cluster_cmap(unique_labels.tolist().index(label))\n",
    "\n",
    "# Plot with distinct colors and noise legend\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=plot_colors, s=5, alpha=0.5)\n",
    "plt.title('2D UMAP Visualization of Movie Clusters')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "\n",
    "# Add legend for noise points\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Noise', markerfacecolor='gray', markersize=5)]\n",
    "plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'cluster_visualization_2d_more_distinct.png'), dpi=300)\n",
    "plt.close()\n",
    "print(f\"Saved 2D cluster visualization with more distinct colors to: {CONFIG['output_dir']}/cluster_visualization_2d_more_distinct.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85c9179-a44a-4ba9-8778-d9105b7225b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
