{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6def9643-461c-40c9-a6dd-f95a058c6e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total genre columns in dataset: 20\n",
      "Top 5 genres in dataset:\n",
      "Genre:Drama: 19750 movies\n",
      "Genre:Comedy: 12380 movies\n",
      "Genre:Thriller: 8275 movies\n",
      "Genre:Action: 6631 movies\n",
      "Genre:Horror: 6335 movies\n",
      "Movies with only one genre: 18774 (41.41%)\n",
      "Movies with only Drama genre: 6667 (14.71%)\n",
      "\n",
      "Genre feature variances:\n",
      "                   Genre  Variance\n",
      "             Genre:Drama  0.000000\n",
      "Genre:(no genres listed)  0.000000\n",
      "            Genre:Comedy  0.000996\n",
      "              Genre:IMAX  0.002022\n",
      "         Genre:Film-Noir  0.002445\n",
      "          Genre:Thriller  0.002597\n",
      "            Genre:Action  0.003420\n",
      "            Genre:Horror  0.003574\n",
      "           Genre:Romance  0.003721\n",
      "           Genre:Musical  0.004012\n",
      "           Genre:Western  0.004267\n",
      "       Genre:Documentary  0.004543\n",
      "             Genre:Crime  0.004575\n",
      "               Genre:War  0.004754\n",
      "         Genre:Adventure  0.004912\n",
      "            Genre:Sci-Fi  0.004920\n",
      "         Genre:Animation  0.005101\n",
      "          Genre:Children  0.005113\n",
      "           Genre:Mystery  0.005118\n",
      "           Genre:Fantasy  0.005120\n",
      "Feature variances (min: 0.0000, max: 0.1985, mean: 0.0516)\n",
      "Warning: Some features have near-zero variance, consider removing them.\n",
      "Running OPTICS clustering...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 10.1 MiB for an array with shape (20609, 64) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 335\u001b[39m\n\u001b[32m    331\u001b[39m         labels = new_labels\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m labels\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m labels = \u001b[43mcluster_optics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# ─── 8. CLUSTER SIZE REPORTING ─────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreport_clusters\u001b[39m(df, labels, genre_cols, config):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 299\u001b[39m, in \u001b[36mcluster_optics\u001b[39m\u001b[34m(latent, config)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning OPTICS clustering...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m optics = OPTICS(\n\u001b[32m    294\u001b[39m     min_samples=config[\u001b[33m'\u001b[39m\u001b[33moptics_min_samples\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    295\u001b[39m     xi=config[\u001b[33m'\u001b[39m\u001b[33moptics_xi\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    296\u001b[39m     metric=config[\u001b[33m'\u001b[39m\u001b[33moptics_metric\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    297\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m    298\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m labels = \u001b[43moptics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# Log initial clustering results\u001b[39;00m\n\u001b[32m    302\u001b[39m n_clusters = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(labels)) - (\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m -\u001b[32m1\u001b[39m \u001b[38;5;129;01min\u001b[39;00m labels \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:719\u001b[39m, in \u001b[36mClusterMixin.fit_predict\u001b[39m\u001b[34m(self, X, y, **kwargs)\u001b[39m\n\u001b[32m    696\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    697\u001b[39m \u001b[33;03mPerform clustering on `X` and returns cluster labels.\u001b[39;00m\n\u001b[32m    698\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    715\u001b[39m \u001b[33;03m    Cluster labels.\u001b[39;00m\n\u001b[32m    716\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    717\u001b[39m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[32m    718\u001b[39m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.labels_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_optics.py:353\u001b[39m, in \u001b[36mOPTICS.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    345\u001b[39m         X.setdiag(X.diagonal())\n\u001b[32m    346\u001b[39m memory = check_memory(\u001b[38;5;28mself\u001b[39m.memory)\n\u001b[32m    348\u001b[39m (\n\u001b[32m    349\u001b[39m     \u001b[38;5;28mself\u001b[39m.ordering_,\n\u001b[32m    350\u001b[39m     \u001b[38;5;28mself\u001b[39m.core_distances_,\n\u001b[32m    351\u001b[39m     \u001b[38;5;28mself\u001b[39m.reachability_,\n\u001b[32m    352\u001b[39m     \u001b[38;5;28mself\u001b[39m.predecessor_,\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m ) = \u001b[43mmemory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_optics_graph\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleaf_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mleaf_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_params\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetric_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_eps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_eps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# Extract clusters from the calculated orders and reachability\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cluster_method == \u001b[33m\"\u001b[39m\u001b[33mxi\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\memory.py:312\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_optics.py:650\u001b[39m, in \u001b[36mcompute_optics_graph\u001b[39m\u001b[34m(X, min_samples, max_eps, metric, p, metric_params, algorithm, leaf_size, n_jobs)\u001b[39m\n\u001b[32m    648\u001b[39m     ordering[ordering_idx] = point\n\u001b[32m    649\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m core_distances_[point] != np.inf:\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m         \u001b[43m_set_reach_dist\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcore_distances_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcore_distances_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreachability_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreachability_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpredecessor_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredecessor_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpoint_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnbrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnbrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetric_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_eps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_eps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.all(np.isinf(reachability_)):\n\u001b[32m    664\u001b[39m     warnings.warn(\n\u001b[32m    665\u001b[39m         (\n\u001b[32m    666\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAll reachability values are inf. Set a larger\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    669\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    670\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_optics.py:711\u001b[39m, in \u001b[36m_set_reach_dist\u001b[39m\u001b[34m(core_distances_, reachability_, predecessor_, point_index, processed, X, nbrs, metric, metric_params, p, max_eps)\u001b[39m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metric == \u001b[33m\"\u001b[39m\u001b[33mminkowski\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mp\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _params:\n\u001b[32m    708\u001b[39m         \u001b[38;5;66;03m# the same logic as neighbors, p is ignored if explicitly set\u001b[39;00m\n\u001b[32m    709\u001b[39m         \u001b[38;5;66;03m# in the dict params\u001b[39;00m\n\u001b[32m    710\u001b[39m         _params[\u001b[33m\"\u001b[39m\u001b[33mp\u001b[39m\u001b[33m\"\u001b[39m] = p\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m     dists = pairwise_distances(P, \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43munproc\u001b[49m\u001b[43m]\u001b[49m, metric, n_jobs=\u001b[38;5;28;01mNone\u001b[39;00m, **_params).ravel()\n\u001b[32m    713\u001b[39m rdists = np.maximum(dists, core_distances_[point_index])\n\u001b[32m    714\u001b[39m np.around(rdists, decimals=np.finfo(rdists.dtype).precision, out=rdists)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 10.1 MiB for an array with shape (20609, 64) and data type float64"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "CONFIG = {\n",
    "    'data_path':               r\"C:\\Users\\PC\\Downloads\\merged_movies (3).csv\",\n",
    "    'embeddings_path':         \"new_embeddings_distilroberta.npy\",\n",
    "    'output_dir':              \"movies_clusters_optics\",\n",
    "    'umap_components':         15,\n",
    "    'umap_n_neighbors':        75,\n",
    "    'umap_min_dist':           0.1,\n",
    "    'umap_metric':             'cosine',\n",
    "    'latent_dim':              64,\n",
    "    'autoencoder_epochs':      50,\n",
    "    'denoising_epochs':        15,\n",
    "    'autoencoder_batch_size':  256,\n",
    "    'denoising_noise_std':     0.05,\n",
    "    'optics_min_samples':      50,  # Minimum points for core point, matches HDBSCAN/DBSCAN\n",
    "    'optics_xi':               0.05,  # Steepness for hierarchical clustering\n",
    "    'optics_metric':           'euclidean',\n",
    "    'noise_reassignment':      True,  # Reassign noise points to nearest cluster\n",
    "    'reassignment_percentile': 90,  # Distance percentile for noise reassignment\n",
    "    'genre_weight':            1.5,\n",
    "    'runtime_weight':          0.1,\n",
    "    'year_weight':             0.3,\n",
    "    'sbert_weight':            1.0,\n",
    "    'silhouette_sample_size':   10000\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ─── 1. LOAD & FILTER DATA ─────────────────────────────────────────────────────\n",
    "def load_and_filter_data(config):\n",
    "    \"\"\"Load and preprocess movie dataset.\"\"\"\n",
    "    df = pd.read_csv(config['data_path']).drop_duplicates('MovieID').reset_index(drop=True)\n",
    "    if 'Genre:(no genres listed)' in df.columns:\n",
    "        df = df[df['Genre:(no genres listed)'] == False].reset_index(drop=True)\n",
    "    df['release_year'] = pd.to_numeric(df.get('release_year', np.nan), errors='coerce')\n",
    "    df['release_year'] = df['release_year'].fillna(df['release_year'].median()).clip(1900, 2025)\n",
    "    df['runtime'] = pd.to_numeric(df['runtime'], errors='coerce').fillna(df['runtime'].median())\n",
    "    \n",
    "    # Log genre distribution\n",
    "    genre_cols = [c for c in df.columns if c.startswith('Genre:')]\n",
    "    print(f\"Total genre columns in dataset: {len(genre_cols)}\")\n",
    "    genre_counts = df[genre_cols].sum().sort_values(ascending=False)\n",
    "    print(\"Top 5 genres in dataset:\")\n",
    "    for i in range(min(5, len(genre_counts))):\n",
    "        print(f\"{genre_counts.index[i]}: {int(genre_counts.iloc[i])} movies\")\n",
    "    single_genre_movies = df[genre_cols].sum(axis=1) == 1\n",
    "    drama_only = (df['Genre:Drama'] == 1) & (df[genre_cols].sum(axis=1) == 1)\n",
    "    print(f\"Movies with only one genre: {single_genre_movies.sum()} ({single_genre_movies.sum()/len(df)*100:.2f}%)\")\n",
    "    print(f\"Movies with only Drama genre: {drama_only.sum()} ({drama_only.sum()/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return df, genre_cols\n",
    "\n",
    "df, genre_cols = load_and_filter_data(CONFIG)\n",
    "\n",
    "# ─── 2. LOAD / COMPUTE SBERT EMBEDDINGS ────────────────────────────────────────\n",
    "def compute_sbert_embeddings(df, config):\n",
    "    \"\"\"Load or compute SBERT embeddings for movie overviews.\"\"\"\n",
    "    cache_path = os.path.join(config['output_dir'], 'sbert_embeddings.npy')\n",
    "    if os.path.exists(cache_path):\n",
    "        return np.load(cache_path)\n",
    "    if os.path.exists(config['embeddings_path']):\n",
    "        embeddings = np.load(config['embeddings_path'])\n",
    "    else:\n",
    "        st_model = SentenceTransformer('distilroberta-base', device=device)\n",
    "        embeddings = st_model.encode(\n",
    "            df['overview'].fillna(''),\n",
    "            batch_size=config['autoencoder_batch_size'],\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        np.save(config['embeddings_path'], embeddings)\n",
    "    np.save(cache_path, embeddings)\n",
    "    return embeddings\n",
    "\n",
    "sbert_embeddings = compute_sbert_embeddings(df, CONFIG)\n",
    "\n",
    "# ─── 3. DENOISING AUTOENCODER ────────────────────────────────────────────────────\n",
    "class DenoiseAE(nn.Module):\n",
    "    \"\"\"Denoising Autoencoder for SBERT embeddings.\"\"\"\n",
    "    def __init__(self, inp, lat):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(inp, 512), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(512, lat), nn.ReLU()\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(lat, 512), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(512, inp)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        return self.dec(z), z\n",
    "\n",
    "def train_denoiser(emb, config):\n",
    "    \"\"\"Train denoising autoencoder and return latent representations.\"\"\"\n",
    "    cache_path = os.path.join(config['output_dir'], 'sbert_denoised.npy')\n",
    "    if os.path.exists(cache_path):\n",
    "        return np.load(cache_path)\n",
    "    \n",
    "    model = DenoiseAE(emb.shape[1], config['latent_dim']).to(device)\n",
    "    opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    loader = DataLoader(\n",
    "        TensorDataset(torch.tensor(emb, dtype=torch.float32)),\n",
    "        batch_size=config['autoencoder_batch_size'], shuffle=True\n",
    "    )\n",
    "    best_loss, patience, counter = float('inf'), 3, 0\n",
    "    for epoch in range(config['denoising_epochs']):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for (batch,) in loader:\n",
    "            batch = batch.to(device)\n",
    "            noisy = batch + torch.randn_like(batch) * config['denoising_noise_std']\n",
    "            recon, _ = model(noisy)\n",
    "            loss = nn.MSELoss()(recon, batch)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "        avg = total / len(loader)\n",
    "        if avg < best_loss:\n",
    "            best_loss, counter = avg, 0\n",
    "            torch.save(model.state_dict(), os.path.join(config['output_dir'], 'denoiser.pth'))\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "    model.load_state_dict(torch.load(os.path.join(config['output_dir'], 'denoiser.pth')))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, den = model(torch.tensor(emb, dtype=torch.float32).to(device))\n",
    "    den = den.cpu().numpy()\n",
    "    np.save(cache_path, den)\n",
    "    return den\n",
    "\n",
    "sbert_denoised = train_denoiser(sbert_embeddings, CONFIG)\n",
    "\n",
    "# ─── 4. UMAP REDUCTION ─────────────────────────────────────────────────────────\n",
    "def reduce_umap(embeddings, config):\n",
    "    \"\"\"Apply UMAP reduction to denoised embeddings.\"\"\"\n",
    "    cache_path = os.path.join(config['output_dir'], 'sbert_reduced.npy')\n",
    "    if os.path.exists(cache_path):\n",
    "        return np.load(cache_path)\n",
    "    \n",
    "    um = umap.UMAP(\n",
    "        n_components=config['umap_components'],\n",
    "        n_neighbors=config['umap_n_neighbors'],\n",
    "        min_dist=config['umap_min_dist'],\n",
    "        metric=config['umap_metric'],\n",
    "        random_state=42\n",
    "    )\n",
    "    umap_emb = um.fit_transform(embeddings)\n",
    "    reduced = MinMaxScaler().fit_transform(umap_emb)\n",
    "    np.save(cache_path, reduced)\n",
    "    \n",
    "    # Visualize UMAP projection\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x=reduced[:10000, 0], y=reduced[:10000, 1], s=10, alpha=0.5)\n",
    "    plt.title('UMAP Projection of Denoised SBERT Embeddings (First 10,000 Movies)')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "    plt.savefig(os.path.join(config['output_dir'], 'umap_projection.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return reduced\n",
    "\n",
    "sbert_reduced = reduce_umap(sbert_denoised, CONFIG)\n",
    "\n",
    "# ─── 5. HYBRID FEATURE ENGINEERING ─────────────────────────────────────────────\n",
    "def make_hybrid(df, sbert_red, genre_cols, config):\n",
    "    \"\"\"Create hybrid features from genres, runtime, year, and SBERT embeddings.\"\"\"\n",
    "    # Compute genre weights\n",
    "    w = np.log(len(df) / (df[genre_cols].sum() + 1))\n",
    "    w = MinMaxScaler().fit_transform(w.values.reshape(-1, 1)).flatten()\n",
    "    w[genre_cols.index('Genre:Drama')] *= 0.7\n",
    "    \n",
    "    # Log genre variances\n",
    "    gf = df[genre_cols].values * w * config['genre_weight']\n",
    "    genre_variances = np.var(gf, axis=0)\n",
    "    variance_df = pd.DataFrame({\n",
    "        'Genre': genre_cols,\n",
    "        'Variance': genre_variances\n",
    "    })\n",
    "    print(\"\\nGenre feature variances:\")\n",
    "    print(variance_df.sort_values('Variance').to_string(index=False))\n",
    "    variance_df.to_csv(os.path.join(config['output_dir'], 'genre_variances.csv'), index=False)\n",
    "    \n",
    "    # Scale other features\n",
    "    y = df['release_year'].values.reshape(-1, 1)\n",
    "    yf = MinMaxScaler().fit_transform(y) * config['year_weight']\n",
    "    r = df['runtime'].values.reshape(-1, 1)\n",
    "    rf = MinMaxScaler().fit_transform(r) * config['runtime_weight']\n",
    "    \n",
    "    hybrid = np.hstack([gf, yf, rf, sbert_red * config['sbert_weight']])\n",
    "    hybrid = MinMaxScaler().fit_transform(hybrid)\n",
    "    \n",
    "    # Validate feature variance\n",
    "    feature_variances = np.var(hybrid, axis=0)\n",
    "    print(f\"Feature variances (min: {feature_variances.min():.4f}, max: {feature_variances.max():.4f}, mean: {feature_variances.mean():.4f})\")\n",
    "    if feature_variances.min() < 1e-4:\n",
    "        print(\"Warning: Some features have near-zero variance, consider removing them.\")\n",
    "    \n",
    "    return hybrid, genre_cols\n",
    "\n",
    "hybrid, valid_genre_cols = make_hybrid(df, sbert_reduced, genre_cols, CONFIG)\n",
    "\n",
    "# ─── 6. COMPRESS HYBRID FEATURES ───────────────────────────────────────────────\n",
    "class HybridAE(nn.Module):\n",
    "    \"\"\"Autoencoder for compressing hybrid features.\"\"\"\n",
    "    def __init__(self, inp, lat):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(inp, 256), nn.ReLU(),\n",
    "            nn.Linear(256, lat), nn.ReLU()\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(lat, 256), nn.ReLU(),\n",
    "            nn.Linear(256, inp)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        return self.dec(z), z\n",
    "\n",
    "def train_hybrid(ftrs, config):\n",
    "    \"\"\"Train hybrid autoencoder and return latent representations.\"\"\"\n",
    "    cache_path = os.path.join(config['output_dir'], 'latent_features.npy')\n",
    "    if os.path.exists(cache_path):\n",
    "        return np.load(cache_path)\n",
    "    \n",
    "    model = HybridAE(ftrs.shape[1], config['latent_dim']).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    loader = DataLoader(\n",
    "        TensorDataset(torch.tensor(ftrs, dtype=torch.float32)),\n",
    "        batch_size=config['autoencoder_batch_size'], shuffle=True\n",
    "    )\n",
    "    best, patience, counter = float('inf'), 5, 0\n",
    "    for epoch in range(config['autoencoder_epochs']):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for (batch,) in loader:\n",
    "            batch = batch.to(device)\n",
    "            recon, _ = model(batch)\n",
    "            loss = nn.MSELoss()(recon, batch)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "        avg = total / len(loader)\n",
    "        if avg < best:\n",
    "            best, counter = avg, 0\n",
    "            torch.save(model.state_dict(), os.path.join(config['output_dir'], 'hybrid_ae.pth'))\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "    model.load_state_dict(torch.load(os.path.join(config['output_dir'], 'hybrid_ae.pth')))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, latent = model(torch.tensor(ftrs, dtype=torch.float32).to(device))\n",
    "    latent = latent.cpu().numpy()\n",
    "    np.save(cache_path, latent)\n",
    "    return latent\n",
    "\n",
    "latent = train_hybrid(hybrid, CONFIG)\n",
    "\n",
    "# ─── 7. OPTICS CLUSTERING ──────────────────────────────────────────────────────\n",
    "def cluster_optics(latent, config):\n",
    "    \"\"\"Apply OPTICS clustering with optional noise reassignment.\"\"\"\n",
    "    print(\"Running OPTICS clustering...\")\n",
    "    optics = OPTICS(\n",
    "        min_samples=config['optics_min_samples'],\n",
    "        xi=config['optics_xi'],\n",
    "        metric=config['optics_metric'],\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    labels = optics.fit_predict(latent)\n",
    "    \n",
    "    # Log initial clustering results\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = np.sum(labels == -1)\n",
    "    print(f\"Initial OPTICS clustering completed: {n_clusters} clusters, {n_noise} noise points ({n_noise/len(labels)*100:.2f}%)\")\n",
    "    \n",
    "    # Reassign noise points to nearest cluster\n",
    "    if config['noise_reassignment'] and n_noise > 0:\n",
    "        print(\"Reassigning noise points...\")\n",
    "        # Compute distances to core points\n",
    "        core_mask = labels != -1\n",
    "        core_points = latent[core_mask]\n",
    "        core_labels = labels[core_mask]\n",
    "        noise_mask = labels == -1\n",
    "        noise_points = latent[noise_mask]\n",
    "        \n",
    "        # Find nearest core point for each noise point\n",
    "        nn = NearestNeighbors(n_neighbors=1, metric=config['optics_metric'], n_jobs=-1)\n",
    "        nn.fit(core_points)\n",
    "        distances, indices = nn.kneighbors(noise_points)\n",
    "        \n",
    "        # Use 90th percentile distance as threshold\n",
    "        distance_threshold = np.percentile(distances, config['reassignment_percentile'])\n",
    "        new_labels = labels.copy()\n",
    "        for i, (dist, idx) in enumerate(zip(distances.flatten(), indices.flatten())):\n",
    "            if dist <= distance_threshold:\n",
    "                new_labels[noise_mask][i] = core_labels[idx]\n",
    "        \n",
    "        # Update noise count\n",
    "        n_noise_new = np.sum(new_labels == -1)\n",
    "        print(f\"Noise reassignment completed: {n_noise_new} noise points remain ({n_noise_new/len(labels)*100:.2f}%)\")\n",
    "        labels = new_labels\n",
    "    \n",
    "    return labels\n",
    "\n",
    "labels = cluster_optics(latent, CONFIG)\n",
    "\n",
    "# ─── 8. CLUSTER SIZE REPORTING ─────────────────────────────────────────────────\n",
    "def report_clusters(df, labels, genre_cols, config):\n",
    "    \"\"\"Generate and save cluster statistics and visualizations.\"\"\"\n",
    "    cluster_sizes = pd.Series(labels[labels != -1]).value_counts().sort_values(ascending=False)\n",
    "    print(\"\\nTop 5 largest clusters (excluding noise):\")\n",
    "    for i in range(5):\n",
    "        if i < len(cluster_sizes):\n",
    "            print(f\"Cluster {cluster_sizes.index[i]}: {cluster_sizes.iloc[i]} movies\")\n",
    "    \n",
    "    # Save largest cluster (if exists)\n",
    "    if len(cluster_sizes) > 0:\n",
    "        largest_cluster_id = cluster_sizes.index[0]\n",
    "        largest_cluster_movies = df[labels == largest_cluster_id][['MovieID', 'title', 'release_year', 'runtime'] + genre_cols]\n",
    "        largest_cluster_movies.to_csv(os.path.join(config['output_dir'], f'cluster_{largest_cluster_id}_movies.csv'), index=False)\n",
    "        print(f\"Saved largest cluster (Cluster {largest_cluster_id}) movies to: {config['output_dir']}/cluster_{largest_cluster_id}_movies.csv\")\n",
    "        genre_counts = largest_cluster_movies[genre_cols].sum().sort_values(ascending=False)\n",
    "        print(f\"\\nTotal genre columns in Cluster {largest_cluster_id}: {len(genre_cols)}\")\n",
    "        print(f\"Non-zero genre columns in Cluster {largest_cluster_id}: {sum(genre_counts > 0)}\")\n",
    "        print(f\"\\nTop 5 genres in Cluster {largest_cluster_id}:\")\n",
    "        for i in range(min(5, len(genre_counts))):\n",
    "            print(f\"{genre_counts.index[i]}: {int(genre_counts.iloc[i])} movies\")\n",
    "    \n",
    "    return cluster_sizes\n",
    "\n",
    "cluster_sizes = report_clusters(df, labels, genre_cols, CONFIG)\n",
    "\n",
    "# ─── 9. FINAL METRICS & VISUALIZATION ──────────────────────────────────────────\n",
    "def compute_metrics_and_visualize(df, latent, labels, cluster_sizes, config):\n",
    "    \"\"\"Compute clustering metrics and generate visualizations.\"\"\"\n",
    "    # Compute metrics (only for clustered points)\n",
    "    clustered_mask = labels != -1\n",
    "    clustered_latent = latent[clustered_mask]\n",
    "    clustered_labels = labels[clustered_mask]\n",
    "    np.random.seed(42)\n",
    "    idx = np.random.choice(len(clustered_latent), min(len(clustered_latent), config['silhouette_sample_size']), replace=False)\n",
    "    sil_score, db_index, ch_score = 0.0, np.inf, 0.0\n",
    "    if len(np.unique(clustered_labels[idx])) > 1:\n",
    "        sil_score = silhouette_score(clustered_latent[idx], clustered_labels[idx])\n",
    "        db_index = davies_bouldin_score(clustered_latent[idx], clustered_labels[idx])\n",
    "        ch_score = calinski_harabasz_score(clustered_latent[idx], clustered_labels[idx])\n",
    "    \n",
    "    # Cluster statistics\n",
    "    num_clusters = len(cluster_sizes)\n",
    "    total_movies = len(labels)\n",
    "    movies_in_clusters = np.sum(clustered_mask)\n",
    "    percentage_in_clusters = (movies_in_clusters / total_movies) * 100\n",
    "    n_noise = total_movies - movies_in_clusters\n",
    "    avg_size = cluster_sizes.mean() if num_clusters > 0 else 0\n",
    "    min_size = cluster_sizes.min() if num_clusters > 0 else 0\n",
    "    max_size = cluster_sizes.max() if num_clusters > 0 else 0\n",
    "    median_size = cluster_sizes.median() if num_clusters > 0 else 0\n",
    "    \n",
    "    # Generate report\n",
    "    report_lines = []\n",
    "    report_lines.append(\"========== Movie Clustering Analysis Report (OPTICS) ==========\\n\")\n",
    "    report_lines.append(\"--- Data Summary ---\\n\")\n",
    "    report_lines.append(f\"Total number of movies: {total_movies}\\n\")\n",
    "    report_lines.append(f\"Clustering performed on: {config['latent_dim']}-dimensional latent space from hybrid features\\n\")\n",
    "    report_lines.append(\"--- Clustering Summary ---\\n\")\n",
    "    report_lines.append(f\"Number of clusters: {num_clusters}\\n\")\n",
    "    report_lines.append(f\"Movies assigned to clusters: {movies_in_clusters} ({percentage_in_clusters:.2f}%)\\n\")\n",
    "    report_lines.append(f\"Noise points: {n_noise} ({n_noise/total_movies*100:.2f}%)\\n\")\n",
    "    report_lines.append(f\"Average cluster size: {avg_size:.1f}\\n\")\n",
    "    report_lines.append(f\"Minimum cluster size: {min_size}\\n\")\n",
    "    report_lines.append(f\"Maximum cluster size: {max_size}\\n\")\n",
    "    report_lines.append(f\"Median cluster size: {median_size}\\n\")\n",
    "    report_lines.append(\"--- Top 5 Largest Clusters ---\\n\")\n",
    "    for i, (cluster, size) in enumerate(cluster_sizes.items()):\n",
    "        if i < 5:\n",
    "            report_lines.append(f\"Cluster {cluster}: {size} movies\\n\")\n",
    "    report_lines.append(\"\\n--- Top 5 Smallest Clusters ---\\n\")\n",
    "    for i, (cluster, size) in enumerate(cluster_sizes.sort_values().items()):\n",
    "        if i < 5:\n",
    "            report_lines.append(f\"Cluster {cluster}: {size} movies\\n\")\n",
    "    \n",
    "    # Sample movies\n",
    "    random.seed(42)\n",
    "    sample_clusters = random.sample(list(cluster_sizes.index), min(3, num_clusters)) if num_clusters > 0 else []\n",
    "    report_lines.append(\"\\n--- Sample Movies from 3 Random Clusters ---\\n\")\n",
    "    for cluster in sample_clusters:\n",
    "        cluster_movies = df[labels == cluster]['title'].tolist()\n",
    "        sample_titles = random.sample(cluster_movies, min(7, len(cluster_movies)))\n",
    "        report_lines.append(f\"\\nCluster {cluster} (size: {cluster_sizes[cluster]}):\\n\")\n",
    "        for title in sample_titles:\n",
    "            report_lines.append(f\" - {title}\\n\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    report_lines.append(\"\\n--- Clustering Quality Metrics (Clustered Points Only) ---\\n\")\n",
    "    report_lines.append(f\"{'Silhouette Score (subsample)':<30}: {sil_score:.4f}\\n\")\n",
    "    report_lines.append(f\"{'Davies–Bouldin Index':<30}: {db_index:.4f}\\n\")\n",
    "    report_lines.append(f\"{'Calinski–Harabasz Score':<30}: {ch_score:.2f}\\n\")\n",
    "    report_lines.append(\"==================================================\\n\")\n",
    "    \n",
    "    # Print and save report\n",
    "    with open(os.path.join(config['output_dir'], 'clustering_report.txt'), 'w') as f:\n",
    "        for line in report_lines:\n",
    "            print(line, end='')\n",
    "            f.write(line)\n",
    "    \n",
    "    # Save cluster assignments\n",
    "    pd.DataFrame({'MovieID': df['MovieID'], 'cluster': labels}).to_csv(\n",
    "        os.path.join(config['output_dir'], 'movie_clusters_optics.csv'), index=False\n",
    "    )\n",
    "    \n",
    "    # Save JSON summary\n",
    "    summary = {\n",
    "        'total_movies': int(total_movies),\n",
    "        'num_clusters': int(num_clusters),\n",
    "        'movies_in_clusters': int(movies_in_clusters),\n",
    "        'percentage_in_clusters': float(percentage_in_clusters),\n",
    "        'noise_points': int(n_noise),\n",
    "        'percentage_noise': float(n_noise/total_movies*100),\n",
    "        'avg_cluster_size': float(avg_size),\n",
    "        'min_cluster_size': int(min_size),\n",
    "        'max_cluster_size': int(max_size),\n",
    "        'median_cluster_size': float(median_size),\n",
    "        'silhouette_score': float(sil_score),\n",
    "        'davies_bouldin_index': float(db_index),\n",
    "        'calinski_harabasz_score': float(ch_score)\n",
    "    }\n",
    "    with open(os.path.join(config['output_dir'], 'clustering_summary.json'), 'w') as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "    \n",
    "    # Visualize clusters in UMAP space\n",
    "    umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    umap_2d = umap_reducer.fit_transform(latent)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(\n",
    "        umap_2d[:, 0], umap_2d[:, 1],\n",
    "        c=labels, cmap='Spectral', s=5, alpha=0.6\n",
    "    )\n",
    "    plt.colorbar(scatter, label='Cluster ID (-1 = Noise)')\n",
    "    plt.title('UMAP Visualization of Movie Clusters (OPTICS)')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "    plt.savefig(os.path.join(config['output_dir'], 'cluster_visualization.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Generate cluster genre distribution table\n",
    "    cluster_genre_dist = []\n",
    "    for cluster in cluster_sizes.index:\n",
    "        cluster_movies = df[labels == cluster]\n",
    "        genre_counts = cluster_movies[genre_cols].sum().to_dict()\n",
    "        genre_counts['Cluster'] = int(cluster)\n",
    "        genre_counts['Size'] = int(cluster_sizes[cluster])\n",
    "        cluster_genre_dist.append(genre_counts)\n",
    "    pd.DataFrame(cluster_genre_dist).to_csv(\n",
    "        os.path.join(config['output_dir'], 'cluster_genre_distribution.csv'), index=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSaved clustering report to: {config['output_dir']}/clustering_report.txt\")\n",
    "    print(f\"Saved cluster assignments to: {config['output_dir']}/movie_clusters_optics.csv\")\n",
    "    print(f\"Saved clustering summary to: {config['output_dir']}/clustering_summary.json\")\n",
    "    print(f\"Saved cluster visualization to: {config['output_dir']}/cluster_visualization.png\")\n",
    "    print(f\"Saved genre distribution to: {config['output_dir']}/cluster_genre_distribution.csv\")\n",
    "    print(f\"Saved genre variances to: {config['output_dir']}/genre_variances.csv\")\n",
    "\n",
    "compute_metrics_and_visualize(df, latent, labels, cluster_sizes, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8e3c56-52a5-4c0c-a16b-b608de00811f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
