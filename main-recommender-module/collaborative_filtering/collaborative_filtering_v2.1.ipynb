{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4d7611",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f27a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterGrid, train_test_split as sklearn_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from surprise import SVD, SVDpp, Dataset, Reader, accuracy, KNNBasic, SlopeOne, CoClustering, NMF, Prediction\n",
    "from surprise.model_selection import GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder , StandardScaler\n",
    "from surprise.accuracy import rmse\n",
    "from joblib import Memory, parallel_backend, dump\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb6e37",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eb5982-6332-4159-b1eb-49659cd700b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../preprocessing/merged_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6dc40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1741c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.drop(columns=[\"Gender\",\"Age\",\"Title\",\"Year\", \"Genres\"])\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"Rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6503030",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_per_user = filtered_df.groupby('UserID')['Rating'].count().reset_index()\n",
    "ratings_per_user.columns = ['user_id', 'num_ratings']\n",
    "print(ratings_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff3e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ratings_per_user['num_ratings'].describe(percentiles=[0.1, 0.5, 0.9])\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"MovieID\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954089a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"UserID\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d29a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_df, _ = sklearn_split(filtered_df, test_size=0.5, random_state=42, stratify=filtered_df['UserID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(filtered_df['Rating'].min(), filtered_df['Rating'].max()))\n",
    "data = Dataset.load_from_df(filtered_df[['UserID', 'MovieID', 'Rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d38c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401ada37",
   "metadata": {},
   "source": [
    "# Test Section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa578eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Surprise Trainset to COO matrix for LightFM\n",
    "def surprise_to_lightfm(trainset):\n",
    "    rows, cols, data = [], [], []\n",
    "    for uid in trainset.all_users():\n",
    "        user_ratings = trainset.ur[uid]\n",
    "        for iid, rating in user_ratings:\n",
    "            rows.append(uid)\n",
    "            cols.append(iid)\n",
    "            data.append(1)  # Implicit feedback\n",
    "    return coo_matrix((data, (rows, cols)))\n",
    "\n",
    "# Compute RMSE and accuracy metrics\n",
    "def compute_metrics(predictions, tolerance=0.5):\n",
    "    actuals = np.array([pred.r_ui for pred in predictions])\n",
    "    preds = np.array([pred.est for pred in predictions])\n",
    "    rmse = np.sqrt(np.mean((preds - actuals) ** 2))\n",
    "    accuracy = np.mean(np.abs(preds - actuals) <= tolerance) * 100\n",
    "    return {'RMSE': rmse, f'Acc (±{tolerance})': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare interactions matrix\n",
    "interactions = surprise_to_lightfm(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and parameter grids\n",
    "models = [\n",
    "    {\n",
    "        'name': 'LightFM-WARP',\n",
    "        'algo': LightFM,\n",
    "        'params': {\n",
    "            'no_components': [20, 50],\n",
    "            'loss': ['warp'],\n",
    "            'learning_rate': [0.005, 0.01],\n",
    "            'item_alpha': [0.02, 0.1, 0.3],\n",
    "            'user_alpha': [0.02, 0.1, 0.3],\n",
    "            'random_state': [42]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'LightFM-BPR',\n",
    "        'algo': LightFM,\n",
    "        'params': {\n",
    "            'no_components': [20, 50],\n",
    "            'loss': ['bpr'],\n",
    "            'learning_rate': [0.005, 0.01],\n",
    "            'item_alpha': [0.02, 0.1, 0.3],\n",
    "            'user_alpha': [0.02, 0.1, 0.3],\n",
    "            'random_state': [42]\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74520f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "best_params_dict = {}\n",
    "for config in models:\n",
    "    print(f\"\\n=== Tuning {config['name']} ===\\n\")\n",
    "    best_score = -np.inf\n",
    "    best_params = {}\n",
    "    for params in ParameterGrid(config['params']):\n",
    "        print(f\"Training {config['name']} with params: {params}\")\n",
    "        model = LightFM(**params)\n",
    "        model.fit(interactions, epochs=10, verbose=False)\n",
    "        score = precision_at_k(model, interactions, k=5).mean()\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "    best_params_dict[config['name']] = best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation\n",
    "results = []\n",
    "for config in models:\n",
    "    print(f\"\\n=== Training {config['name']} ===\\n\")\n",
    "    model_result = {'Model': config['name']}\n",
    "    best_params = best_params_dict[config['name']]\n",
    "    model = LightFM(**best_params)\n",
    "    model.fit(interactions, epochs=30)\n",
    "\n",
    "    # Generate valid test indices\n",
    "    valid_indices = []\n",
    "    test_user_ids = []\n",
    "    test_item_ids = []\n",
    "    for idx, (uid, iid, rating) in enumerate(testset):\n",
    "        try:\n",
    "            u_inner = trainset.to_inner_uid(uid)\n",
    "            i_inner = trainset.to_inner_iid(iid)\n",
    "            valid_indices.append(idx)\n",
    "            test_user_ids.append(u_inner)\n",
    "            test_item_ids.append(i_inner)\n",
    "        except ValueError:\n",
    "            continue  # Skip cold-start users/items\n",
    "\n",
    "    # Predict and scale to rating range\n",
    "    preds = model.predict(test_user_ids, test_item_ids)\n",
    "    min_rating, max_rating = filtered_df['Rating'].min(), filtered_df['Rating'].max()\n",
    "    min_pred, max_pred = np.min(preds), np.max(preds)\n",
    "    if max_pred != min_pred:  # Avoid division by zero\n",
    "        scaled_preds = min_rating + (preds - min_pred) * (max_rating - min_rating) / (max_pred - min_pred)\n",
    "    else:\n",
    "        scaled_preds = preds  # Fallback if all predictions are the same\n",
    "\n",
    "    # Create Prediction objects\n",
    "    predictions = [\n",
    "        Prediction(\n",
    "            uid=testset[idx][0],\n",
    "            iid=testset[idx][1],\n",
    "            r_ui=testset[idx][2],\n",
    "            est=float(scaled_preds[j]),\n",
    "            details=None,\n",
    "        )\n",
    "        for j, idx in enumerate(valid_indices)\n",
    "    ]\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = compute_metrics(predictions)\n",
    "    precision = precision_at_k(model, interactions, k=5).mean()\n",
    "    model_result.update(metrics)\n",
    "    model_result.update({'Precision@5': precision, 'Best Params': str(best_params)})\n",
    "    results.append(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc87454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Final Results ===\\n\")\n",
    "styled_df = results_df.style.format({\n",
    "    'RMSE': '{:.3f}',\n",
    "    'Acc (±0.5)': '{:.1f}%',\n",
    "    'Precision@5': '{:.3f}',\n",
    "    'Best Params': lambda x: x.replace(', ', ',\\n')\n",
    "}).set_properties(**{'text-align': 'left'})\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeafef7",
   "metadata": {},
   "source": [
    "# Test Section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6307ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightFM(no_components=20, loss='bpr', learning_rate=0.005, item_alpha=0.02, user_alpha=0.02, random_state=42)\n",
    "model.fit(interactions, epochs=30, num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_user_ids, test_item_ids, num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b33e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = precision_at_k(model, interactions, k=5, num_threads=4)\n",
    "print(f\"Precision at k=5: {score.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06ee873",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([pred.est for pred in predictions])\n",
    "actuals = np.array([pred.r_ui for pred in predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2857a35c",
   "metadata": {},
   "source": [
    "# Test Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa389a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RMSE and accuracy metrics\n",
    "def compute_metrics(predictions, tolerance=0.5):\n",
    "    actuals = np.array([pred.r_ui for pred in predictions])\n",
    "    preds = np.array([pred.est for pred in predictions])\n",
    "    rmse_val = np.sqrt(np.mean((preds - actuals) ** 2))\n",
    "    accuracy = np.mean(np.abs(preds - actuals) <= tolerance) * 100\n",
    "    return {'RMSE': rmse_val, f'Acc (±{tolerance})': accuracy}\n",
    "\n",
    "# Compute precision@k for top-N recommendations\n",
    "def compute_precision_at_k(predictions, k=5, threshold=3.5):\n",
    "    user_est_true = {}\n",
    "    for pred in predictions:\n",
    "        uid, iid, true_r, est, _ = pred\n",
    "        if uid not in user_est_true:\n",
    "            user_est_true[uid] = []\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    precisions = []\n",
    "    for uid, ratings in user_est_true.items():\n",
    "        ratings.sort(key=lambda x: x[0], reverse=True)  # Sort by predicted rating\n",
    "        top_k = [r[1] >= threshold for r in ratings[:k]]  # True ratings >= threshold\n",
    "        if top_k:\n",
    "            precisions.append(sum(top_k) / len(top_k))\n",
    "    \n",
    "    return np.mean(precisions) if precisions else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7af775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = model.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "metrics = compute_metrics(predictions)\n",
    "precision_k = compute_precision_at_k(predictions, k=5, threshold=3.5)\n",
    "metrics.update({'Precision@5': precision_k, 'Best Params': str(best_params)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2049d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "results = [metrics]\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== CF Module Results (SVD) ===\\n\")\n",
    "styled_df = results_df.style.format({\n",
    "    'RMSE': '{:.3f}',\n",
    "    'Acc (±0.5)': '{:.1f}%',\n",
    "    'Precision@5': '{:.3f}',\n",
    "    'Best Params': lambda x: x.replace(', ', ',\\n')\n",
    "}).set_properties(**{'text-align': 'left'})\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8578c380",
   "metadata": {},
   "source": [
    "# Test Section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf556f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RMSE and accuracy metrics\n",
    "def compute_metrics(predictions, tolerance=0.5):\n",
    "    actuals = np.array([pred.r_ui for pred in predictions])\n",
    "    preds = np.array([pred.est for pred in predictions])\n",
    "    rmse_val = np.sqrt(np.mean((preds - actuals) ** 2))\n",
    "    accuracy = np.mean(np.abs(preds - actuals) <= tolerance) * 100\n",
    "    return {'RMSE': rmse_val, f'Acc (±{tolerance})': accuracy}\n",
    "\n",
    "# Compute precision@k\n",
    "def compute_precision_at_k(predictions, k=5, threshold=3.5):\n",
    "    user_est_true = {}\n",
    "    for pred in predictions:\n",
    "        uid, iid, true_r, est, _ = pred\n",
    "        if uid not in user_est_true:\n",
    "            user_est_true[uid] = []\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    precisions = []\n",
    "    for uid, ratings in user_est_true.items():\n",
    "        ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        top_k = [r[1] >= threshold for r in ratings[:k]]\n",
    "        if top_k:\n",
    "            precisions.append(sum(top_k) / len(top_k))\n",
    "    \n",
    "    return np.mean(precisions) if precisions else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85392b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_factors': [50, 100],\n",
    "    'n_epochs': [20, 30],\n",
    "    'lr_all': [0.005, 0.01],\n",
    "    'reg_all': [0.02, 0.1]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(SVDpp, param_grid, measures=['rmse'], cv=5, n_jobs=-1)\n",
    "gs.fit(data)\n",
    "\n",
    "# Best parameters\n",
    "best_params = gs.best_params['rmse']\n",
    "print(f\"Best RMSE: {gs.best_score['rmse']:.4f}\")\n",
    "print(f\"Best params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f985299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model\n",
    "model = SVDpp(**best_params, random_state=42)\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397867ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = model.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "metrics = compute_metrics(predictions)\n",
    "precision_k = compute_precision_at_k(predictions, k=5, threshold=3.5)\n",
    "metrics.update({'Precision@5': precision_k, 'Best Params': str(best_params)})\n",
    "\n",
    "# Display results\n",
    "results = [metrics]\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== CF Module Results (SVDpp) ===\\n\")\n",
    "styled_df = results_df.style.format({\n",
    "    'RMSE': '{:.3f}',\n",
    "    'Acc (±0.5)': '{:.1f}%',\n",
    "    'Precision@5': '{:.3f}',\n",
    "    'Best Params': lambda x: x.replace(', ', ',\\n')\n",
    "}).set_properties(**{'text-align': 'left'})\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93534d36",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35861455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and parameter grids\n",
    "models = [\n",
    "    {\n",
    "        'name': 'LightFM-WARP',\n",
    "        'algo': LightFM,\n",
    "        'params': {\n",
    "            'no_components': [20, 50],\n",
    "            'loss': ['warp'],\n",
    "            'learning_rate': [0.01, 0.03],\n",
    "            'item_alpha': [0.02, 0.1],\n",
    "            'user_alpha': [0.02, 0.1],\n",
    "            'random_state': [42]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'LightFM-BPR',\n",
    "        'algo': LightFM,\n",
    "        'params': {\n",
    "            'no_components': [20, 50],\n",
    "            'loss': ['bpr'],\n",
    "            'learning_rate': [0.01, 0.03],\n",
    "            'item_alpha': [0.02, 0.1],\n",
    "            'user_alpha': [0.02, 0.1],\n",
    "            'random_state': [42]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'SVD',\n",
    "        'algo': SVD,\n",
    "        'params': {\n",
    "            'n_factors': [50, 100, 150],\n",
    "            'n_epochs': [20, 30],\n",
    "            'lr_all': [0.005, 0.01],\n",
    "            'reg_all': [0.02, 0.1]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'KNNBasic',\n",
    "        'algo': KNNBasic,\n",
    "        'params': {\n",
    "            'k': [20, 40],\n",
    "            'sim_options': {\n",
    "                'name': ['msd', 'pearson'],\n",
    "                'user_based': [False]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'NMF',\n",
    "        'algo': NMF,\n",
    "        'params': {\n",
    "            'n_factors': [10, 15],\n",
    "            'n_epochs': [50, 100]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'CoClustering',\n",
    "        'algo': CoClustering,\n",
    "        'params': {\n",
    "            'n_cltr_u': [3, 5],\n",
    "            'n_cltr_i': [3, 5],\n",
    "            'n_epochs': [20, 30]\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c144c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surprise_to_lightfm(trainset):\n",
    "    \"\"\"Convert Surprise Trainset to COO matrix for LightFM\"\"\"\n",
    "    rows, cols, data = [], [], []\n",
    "    for uid in trainset.all_users():\n",
    "        user_ratings = trainset.ur[uid]\n",
    "        for iid, rating in user_ratings:\n",
    "            rows.append(uid)\n",
    "            cols.append(iid)\n",
    "            data.append(1)  # Use 1 for implicit feedback\n",
    "    return coo_matrix((data, (rows, cols))), trainset.n_users, trainset.n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83abda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions, _, _ = surprise_to_lightfm(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your training loop:\n",
    "for model_config in models:\n",
    "    if 'LightFM' in model_config['name']:\n",
    "        # LightFM handling\n",
    "        #interactions, _, _ = surprise_to_lightfm(trainset)\n",
    "        \n",
    "        # Hyperparameter tuning\n",
    "        best_score = -np.inf\n",
    "        best_params = {}\n",
    "        for params in ParameterGrid(model_config['params']):\n",
    "            print(f\"Training {model_config['name']} with params: {params}\")\n",
    "            model = LightFM(**params)\n",
    "            model.fit(interactions, epochs=10, verbose=False)\n",
    "            score = precision_at_k(model, interactions, k=5).mean()\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "                \n",
    "        # Final training\n",
    "        print(f\"[Final] Training {model_config['name']} with params: {params}\")\n",
    "        final_model = LightFM(**best_params)\n",
    "        final_model.fit(interactions, epochs=20)\n",
    "        \n",
    "        # Generate predictions (example for LightFM)\n",
    "        user_ids = np.arange(interactions.shape[0])\n",
    "\n",
    "    else:\n",
    "    # Original Surprise handling\n",
    "        print(f\"Training {model_config['name']} with params: {params}\")\n",
    "        gs = GridSearchCV(\n",
    "            model_config['algo'],\n",
    "            model_config['params'],\n",
    "            measures=['rmse'],\n",
    "            cv=5\n",
    "        )\n",
    "        gs.fit(data)\n",
    "        best_model = gs.best_estimator['rmse']\n",
    "        best_model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845292d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert testset to LightFM-compatible format\n",
    "test_user_ids = [trainset.to_inner_uid(uid) for (uid, _, _) in testset]\n",
    "test_item_ids = [trainset.to_inner_iid(iid) for (_, iid, _) in testset]\n",
    "\n",
    "# Generate predictions only for test pairs\n",
    "test_preds = final_model.predict(test_user_ids, test_item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = final_model.predict(user_ids, np.arange(interactions.shape[1]))\n",
    "#print(\"Train precision: %.2f\" % precision_at_k(model, interactions, k=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b4f57",
   "metadata": {},
   "source": [
    "best_params = {}\n",
    "\n",
    "for model_config in models:\n",
    "    print(f\"\\n=== Tuning {model_config['name']} ===\")\n",
    "    gs = GridSearchCV(\n",
    "        model_config['algo'],\n",
    "        model_config['params'],\n",
    "        measures=['rmse'],\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        pre_dispatch='2*n_jobs'\n",
    "    )\n",
    "    gs.fit(data)\n",
    "    best_params[model_config['name']] = gs.best_params['rmse']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f81d7b",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f2a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for model_config in models:\n",
    "    if 'LightFM' in model_config['name']:\n",
    "        continue  # Skip LightFM for now\n",
    "    print(f\"\\n=== Training {model_config['name']} ===\")\n",
    "    \n",
    "    # Initialize with best params\n",
    "    model = model_config['algo'](**best_params[model_config['name']])\n",
    "    model.fit(trainset)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = model.test(testset)\n",
    "    preds = np.array([pred.est for pred in predictions])\n",
    "    actuals = np.array([pred.r_ui for pred in predictions])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(np.mean((preds - actuals) ** 2))\n",
    "    tol_1 = np.mean(np.abs(preds - actuals) <= 1) * 100\n",
    "    tol_05 = np.mean(np.abs(preds - actuals) <= 0.5) * 100\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_config['name'],\n",
    "        'Best Params': best_params[model_config['name']],\n",
    "        'RMSE': rmse,\n",
    "        'Acc (±1)': tol_1,\n",
    "        'Acc (±0.5)': tol_05\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80316310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each model's predictions:\n",
    "tolerance = 1\n",
    "stricter_tolerance = 0.5\n",
    "\n",
    "for model_result in results:\n",
    "    model_name = model_result['Model']\n",
    "    print(f\"\\n{model_name} Accuracy:\")\n",
    "    print(f\"Within ±{tolerance} Stars: {model_result['Acc (±1)']:.2f}%\")\n",
    "    print(f\"Within ±{stricter_tolerance} Stars: {model_result['Acc (±0.5)']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06806afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds = np.array([pred.est for pred in test_preds]).reshape(-1, 1)\n",
    "#actuals = np.array([pred.r_ui for pred in test_preds]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba540de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print predictions\n",
    "#for pred in test_preds:\n",
    "#    print(f\"Predicted={pred.est:.2f}, Actual={pred.r_ui}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f51e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Optional: Formatting for better display\n",
    "results_df.style.format({\n",
    "    'RMSE': '{:.4f}',\n",
    "    'Acc (±1)': '{:.2f}%',\n",
    "    'Acc (±0.5)': '{:.2f}%'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tolerance (e.g., predictions within ±1 stars are \"correct\")\n",
    "tolerance = 1\n",
    "correct = np.abs(preds - actuals) <= tolerance\n",
    "test_accuracy = np.mean(correct) * 100\n",
    "\n",
    "# Define stricter tolerance (e.g., predictions within ±0.5 stars are \"correct\")\n",
    "stricter_tolerance = 0.5\n",
    "s_correct = np.abs(preds - actuals) <= stricter_tolerance\n",
    "s_test_accuracy = np.mean(s_correct) * 100\n",
    "\n",
    "print(f\"Accuracy (Within ±{tolerance} Stars): {test_accuracy:.2f}%\")\n",
    "print(f\"Accuracy (Within ±{stricter_tolerance} Stars): {s_test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0612cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "dump(model, '../models/cf_model.pkl')  # Or use .joblib extension\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa8235",
   "metadata": {},
   "source": [
    "# Model Training with 10M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Upgrade\n",
    "beeg_data = pd.read_csv(r\"K:\\MachineProject\\Data\\ml-32m\\ratings.dat\", sep='::', engine='python', names=['UserID', 'MovieID', 'Rating', 'Timestamp'])\n",
    "\n",
    "# Drop the Timestamp column\n",
    "beeg_data = beeg_data.drop('Timestamp', axis=1)\n",
    "beeg_data.columns = ['UserID', 'MovieID', 'Rating']\n",
    "beeg_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd291d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_per_user = beeg_data.groupby('UserID')['Rating'].count().reset_index()\n",
    "ratings_per_user.columns = ['user_id', 'num_ratings']\n",
    "print(ratings_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4844132",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ratings_per_user['num_ratings'].describe(percentiles=[0.1, 0.5, 0.9])\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d9cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "beeg_data[\"Rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "beeg_data[\"MovieID\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d651be",
   "metadata": {},
   "outputs": [],
   "source": [
    "beeg_data[\"UserID\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c1c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "beeg_data['UserID'] = beeg_data['UserID'].astype('int32')\n",
    "beeg_data['MovieID'] = beeg_data['MovieID'].astype('int32')\n",
    "beeg_data['Rating'] = beeg_data['Rating'].astype('float16')\n",
    "\n",
    "beeg_data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify by user_id (ensure all users are represented)\n",
    "subsampled_df, _ = sklearn_split(\n",
    "    beeg_data,\n",
    "    test_size=0.5,\n",
    "    stratify=beeg_data['UserID'],  # Preserve user distribution\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab49a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(beeg_data['Rating'].min(), beeg_data['Rating'].max()))\n",
    "data = Dataset.load_from_df(beeg_data[['UserID', 'MovieID', 'Rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(location='./cache', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8a3a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_factors': [50, 100],  # Test latent dimensions\n",
    "    'n_epochs': [20, 30],\n",
    "    'lr_all': [0.005, 0.01],\n",
    "    'reg_all': [0.02, 0.1]\n",
    "}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    SVD,\n",
    "    param_grid,\n",
    "    measures=['rmse'],\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    ")\n",
    "gs.fit(data)\n",
    "\n",
    "# Best RMSE score and params\n",
    "print(f\"Best RMSE: {gs.best_score['rmse']}\")\n",
    "print(f\"Best params: {gs.best_params['rmse']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876697af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09545c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVD(n_factors=50, n_epochs=20, lr_all=0.005, reg_all=0.02, random_state=42)\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b27573",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.test(testset)\n",
    "accuracy.rmse(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e7b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([pred.est for pred in test_preds]).reshape(-1, 1)\n",
    "actuals = np.array([pred.r_ui for pred in test_preds]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d256c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tolerance (e.g., predictions within ±1 stars are \"correct\")\n",
    "tolerance = 1\n",
    "correct = np.abs(preds - actuals) <= tolerance\n",
    "test_accuracy = np.mean(correct) * 100\n",
    "\n",
    "# Define stricter tolerance (e.g., predictions within ±0.5 stars are \"correct\")\n",
    "stricter_tolerance = 0.5\n",
    "s_correct = np.abs(preds - actuals) <= stricter_tolerance\n",
    "s_test_accuracy = np.mean(s_correct) * 100\n",
    "\n",
    "print(f\"Accuracy (Within ±{tolerance} Stars): {test_accuracy:.2f}%\")\n",
    "print(f\"Accuracy (Within ±{stricter_tolerance} Stars): {s_test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ad150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "dump(model, '../models/cf_model_2.pkl')  # Or use .joblib extension\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab4e897",
   "metadata": {},
   "source": [
    "# Model Training with 32M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645626c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Upgrade\n",
    "beegar_data = pd.read_csv(r\"K:\\MachineProject\\Data\\ml-32m\\ratings.csv\")\n",
    "\n",
    "# Drop the Timestamp column\n",
    "beegar_data = beegar_data.drop('timestamp', axis=1)\n",
    "beegar_data.columns = ['UserID', 'MovieID', 'Rating']\n",
    "beegar_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_per_user = beegar_data.groupby('UserID')['Rating'].count().reset_index()\n",
    "ratings_per_user.columns = ['user_id', 'num_ratings']\n",
    "print(ratings_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118825b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ratings_per_user['num_ratings'].describe(percentiles=[0.1, 0.5, 0.9])\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee1c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "beegar_data[\"Rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "beegar_data[\"MovieID\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82463015",
   "metadata": {},
   "outputs": [],
   "source": [
    "beegar_data[\"UserID\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "beegar_data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "beegar_data['UserID'] = beegar_data['UserID'].astype('int32')\n",
    "beegar_data['MovieID'] = beegar_data['MovieID'].astype('int32')\n",
    "beegar_data['Rating'] = beegar_data['Rating'].astype('float16')\n",
    "\n",
    "beegar_data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46ee50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify by user_id (ensure all users are represented)\n",
    "subsampled_df, _ = sklearn_split(\n",
    "    beegar_data,\n",
    "    test_size=0.5,\n",
    "    stratify=beegar_data['UserID'],  # Preserve user distribution\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f50baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(beegar_data['Rating'].min(), beegar_data['Rating'].max()))\n",
    "data = Dataset.load_from_df(beegar_data[['UserID', 'MovieID', 'Rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93440a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(location='./cache', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8851ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_factors': [50, 100],  # Test latent dimensions\n",
    "    'n_epochs': [20, 30],\n",
    "    'lr_all': [0.005, 0.01],\n",
    "    'reg_all': [0.02, 0.1]\n",
    "}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    SVD,\n",
    "    param_grid,\n",
    "    measures=['rmse'],\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    ")\n",
    "gs.fit(data)\n",
    "\n",
    "# Best RMSE score and params\n",
    "print(f\"Best RMSE: {gs.best_score['rmse']}\")\n",
    "print(f\"Best params: {gs.best_params['rmse']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5fffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71fabf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVD(n_factors=50, n_epochs=20, lr_all=0.005, reg_all=0.02, random_state=42)\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e87ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.test(testset)\n",
    "accuracy.rmse(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce5511",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([pred.est for pred in test_preds]).reshape(-1, 1)\n",
    "actuals = np.array([pred.r_ui for pred in test_preds]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e7299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tolerance (e.g., predictions within ±1 stars are \"correct\")\n",
    "tolerance = 1\n",
    "correct = np.abs(preds - actuals) <= tolerance\n",
    "test_accuracy = np.mean(correct) * 100\n",
    "\n",
    "# Define stricter tolerance (e.g., predictions within ±0.5 stars are \"correct\")\n",
    "stricter_tolerance = 0.5\n",
    "s_correct = np.abs(preds - actuals) <= stricter_tolerance\n",
    "s_test_accuracy = np.mean(s_correct) * 100\n",
    "\n",
    "print(f\"Accuracy (Within ±{tolerance} Stars): {test_accuracy:.2f}%\")\n",
    "print(f\"Accuracy (Within ±{stricter_tolerance} Stars): {s_test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "dump(model, '../models/cf_model_2.pkl')  # Or use .joblib extension\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLab10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
