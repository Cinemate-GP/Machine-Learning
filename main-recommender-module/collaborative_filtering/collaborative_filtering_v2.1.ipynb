{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4d7611",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f27a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterGrid, train_test_split as sklearn_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from surprise import SVD, SVDpp, Dataset, Reader, accuracy, KNNBasic, SlopeOne, CoClustering, NMF, Prediction\n",
    "from surprise.model_selection import GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder , StandardScaler\n",
    "from surprise.accuracy import rmse\n",
    "from joblib import Memory, parallel_backend, dump\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb6e37",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59eb5982-6332-4159-b1eb-49659cd700b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>One Flew Over the Cuckoo's Nest</td>\n",
       "      <td>[8]</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>James and the Giant Peach</td>\n",
       "      <td>[3, 4, 12]</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>My Fair Lady</td>\n",
       "      <td>[12, 14]</td>\n",
       "      <td>1964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Erin Brockovich</td>\n",
       "      <td>[8]</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Bug's Life, A</td>\n",
       "      <td>[3, 4, 5]</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  Gender  Age                            Title  \\\n",
       "0       1     1193       5       0    1  One Flew Over the Cuckoo's Nest   \n",
       "1       1      661       3       0    1        James and the Giant Peach   \n",
       "2       1      914       3       0    1                     My Fair Lady   \n",
       "3       1     3408       4       0    1                  Erin Brockovich   \n",
       "4       1     2355       5       0    1                    Bug's Life, A   \n",
       "\n",
       "       Genres  Year  \n",
       "0         [8]  1975  \n",
       "1  [3, 4, 12]  1996  \n",
       "2    [12, 14]  1964  \n",
       "3         [8]  2000  \n",
       "4   [3, 4, 5]  1998  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../preprocessing/merged_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6dc40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1741c975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating\n",
       "0       1     1193       5\n",
       "1       1      661       3\n",
       "2       1      914       3\n",
       "3       1     3408       4\n",
       "4       1     2355       5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = df.drop(columns=[\"Gender\",\"Age\",\"Title\",\"Year\", \"Genres\"])\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6277a887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating\n",
       "4    348971\n",
       "3    261197\n",
       "5    226310\n",
       "2    107557\n",
       "1     56174\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df[\"Rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6503030",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_per_user = filtered_df.groupby('UserID')['Rating'].count().reset_index()\n",
    "ratings_per_user.columns = ['user_id', 'num_ratings']\n",
    "print(ratings_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff3e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ratings_per_user['num_ratings'].describe(percentiles=[0.1, 0.5, 0.9])\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"MovieID\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954089a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"UserID\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d29a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_df, _ = sklearn_split(filtered_df, test_size=0.5, random_state=42, stratify=filtered_df['UserID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ddc7bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating\n",
       "0       1       17     4.0\n",
       "1       1       25     1.0\n",
       "2       1       29     2.0\n",
       "3       1       30     5.0\n",
       "4       1       32     5.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset Upgrade\n",
    "beegar_data = pd.read_csv(r\"~/Downloads/ratings.csv\")\n",
    "\n",
    "# Drop the Timestamp column\n",
    "beegar_data = beegar_data.drop('timestamp', axis=1)\n",
    "beegar_data.columns = ['UserID', 'MovieID', 'Rating']\n",
    "beegar_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a37e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "beegar_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf4d878a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32000204 entries, 0 to 32000203\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   UserID   int32  \n",
      " 1   MovieID  int32  \n",
      " 2   Rating   float16\n",
      "dtypes: float16(1), int32(2)\n",
      "memory usage: 305.2 MB\n"
     ]
    }
   ],
   "source": [
    "beegar_data['UserID'] = beegar_data['UserID'].astype('int32')\n",
    "beegar_data['MovieID'] = beegar_data['MovieID'].astype('int32')\n",
    "beegar_data['Rating'] = beegar_data['Rating'].astype('float16')\n",
    "\n",
    "beegar_data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(beegar_data['Rating'].min(), beegar_data['Rating'].max()))\n",
    "data = Dataset.load_from_df(beegar_data[['UserID', 'MovieID', 'Rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d38c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = sklearn_split(beegar_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401ada37",
   "metadata": {},
   "source": [
    "# Test Section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa578eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Surprise Trainset to COO matrix for LightFM\n",
    "def surprise_to_lightfm(trainset):\n",
    "    rows, cols, data = [], [], []\n",
    "    for uid in trainset.all_users():\n",
    "        user_ratings = trainset.ur[uid]\n",
    "        for iid, rating in user_ratings:\n",
    "            rows.append(uid)\n",
    "            cols.append(iid)\n",
    "            data.append(1)  # Implicit feedback\n",
    "    return coo_matrix((data, (rows, cols)))\n",
    "\n",
    "# Compute RMSE and accuracy metrics\n",
    "def compute_metrics(predictions, tolerance=0.5):\n",
    "    actuals = np.array([pred.r_ui for pred in predictions])\n",
    "    preds = np.array([pred.est for pred in predictions])\n",
    "    rmse = np.sqrt(np.mean((preds - actuals) ** 2))\n",
    "    accuracy = np.mean(np.abs(preds - actuals) <= tolerance) * 100\n",
    "    return {'RMSE': rmse, f'Acc (±{tolerance})': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51c5962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to COO matrix for LightFM\n",
    "def df_to_lightfm(train_df):\n",
    "    # Map UserID and MovieID to consecutive indices\n",
    "    user_ids = train_df['UserID'].unique()\n",
    "    item_ids = train_df['MovieID'].unique()\n",
    "    user_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "    item_map = {iid: idx for idx, iid in enumerate(item_ids)}\n",
    "    \n",
    "    # Prepare data for coo_matrix\n",
    "    rows = [user_map[uid] for uid in train_df['UserID']]\n",
    "    cols = [item_map[iid] for iid in train_df['MovieID']]\n",
    "    data = [1] * len(train_df)  # Binary implicit feedback\n",
    "    \n",
    "    # Create sparse matrix\n",
    "    interactions = coo_matrix((data, (rows, cols)), shape=(len(user_ids), len(item_ids)))\n",
    "    return interactions, user_map, item_map\n",
    "\n",
    "# Compute RMSE and accuracy metrics\n",
    "def compute_metrics(predictions, tolerance=0.5):\n",
    "    actuals = np.array([pred.r_ui for pred in predictions])\n",
    "    preds = np.array([pred.est for pred in predictions])\n",
    "    rmse = np.sqrt(np.mean((preds - actuals) ** 2))\n",
    "    accuracy = np.mean(np.abs(preds - actuals) <= tolerance) * 100\n",
    "    return {'RMSE': rmse, f'Acc (±{tolerance})': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c1942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data using sklearn\n",
    "train_df, test_df = sklearn_split(beegar_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af0d2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare interactions matrix\n",
    "interactions, user_map, item_map = df_to_lightfm(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare interactions matrix\n",
    "interactions = surprise_to_lightfm(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and parameter grids\n",
    "models = [\n",
    "    {\n",
    "        'name': 'LightFM-WARP',\n",
    "        'algo': LightFM,\n",
    "        'params': {\n",
    "            'no_components': [20, 50],\n",
    "            'loss': ['warp'],\n",
    "            'learning_rate': [0.005, 0.01],\n",
    "            'item_alpha': [0.02, 0.1, 0.3],\n",
    "            'user_alpha': [0.02, 0.1, 0.3],\n",
    "            'random_state': [42]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'LightFM-BPR',\n",
    "        'algo': LightFM,\n",
    "        'params': {\n",
    "            'no_components': [20, 50],\n",
    "            'loss': ['bpr'],\n",
    "            'learning_rate': [0.005, 0.01],\n",
    "            'item_alpha': [0.02, 0.1, 0.3],\n",
    "            'user_alpha': [0.02, 0.1, 0.3],\n",
    "            'random_state': [42]\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74520f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "best_params_dict = {}\n",
    "for config in models:\n",
    "    print(f\"\\n=== Tuning {config['name']} ===\\n\")\n",
    "    best_score = -np.inf\n",
    "    best_params = {}\n",
    "    for params in ParameterGrid(config['params']):\n",
    "        print(f\"Training {config['name']} with params: {params}\")\n",
    "        model = LightFM(**params)\n",
    "        model.fit(interactions, epochs=10, verbose=False)\n",
    "        score = precision_at_k(model, interactions, k=5).mean()\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "    best_params_dict[config['name']] = best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f55188f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodels = {\n",
    "    'LightFM-WARP': LightFM(loss='warp', no_components=20, learning_rate=0.005, item_alpha=0.1, user_alpha=0.02, random_state=42),\n",
    "    'LightFM-BPR': LightFM(loss='bpr', no_components=20, learning_rate=0.005, item_alpha=0.02, user_alpha=0.02, random_state=42),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation\n",
    "results = []\n",
    "for model_name, model_instance in lmodels.items():\n",
    "    print(f\"\\n=== Training {model_name} ===\\n\")\n",
    "    model_result = {'Model': model_name}\n",
    "    model_instance.fit(interactions, epochs=30)\n",
    "\n",
    "    # Generate valid test indices\n",
    "    valid_indices = []\n",
    "    test_user_ids = []\n",
    "    test_item_ids = []\n",
    "    for idx, (uid, iid, rating) in enumerate(testset):\n",
    "        try:\n",
    "            u_inner = trainset.to_inner_uid(uid)\n",
    "            i_inner = trainset.to_inner_iid(iid)\n",
    "            valid_indices.append(idx)\n",
    "            test_user_ids.append(u_inner)\n",
    "            test_item_ids.append(i_inner)\n",
    "        except ValueError:\n",
    "            continue  # Skip cold-start users/items\n",
    "\n",
    "    # Predict and scale to rating range\n",
    "    preds = model_instance.predict(test_user_ids, test_item_ids)\n",
    "    min_rating, max_rating = beegar_data['Rating'].min(), beegar_data['Rating'].max()\n",
    "    min_pred, max_pred = np.min(preds), np.max(preds)\n",
    "    if max_pred != min_pred:  # Avoid division by zero\n",
    "        scaled_preds = min_rating + (preds - min_pred) * (max_rating - min_rating) / (max_pred - min_pred)\n",
    "    else:\n",
    "        scaled_preds = preds  # Fallback if all predictions are the same\n",
    "\n",
    "    # Create Prediction objects\n",
    "    predictions = [\n",
    "        Prediction(\n",
    "            uid=testset[idx][0],\n",
    "            iid=testset[idx][1],\n",
    "            r_ui=testset[idx][2],\n",
    "            est=float(scaled_preds[j]),\n",
    "            details=None,\n",
    "        )\n",
    "        for j, idx in enumerate(valid_indices)\n",
    "    ]\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = compute_metrics(predictions)\n",
    "    precision = precision_at_k(model_instance, interactions, k=5).mean()\n",
    "    model_result.update(metrics)\n",
    "    model_result.update({'Precision@5': precision})\n",
    "    results.append(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc87454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Final Results ===\\n\")\n",
    "styled_df = results_df.style.format({\n",
    "    'RMSE': '{:.3f}',\n",
    "    'Acc (±0.5)': '{:.1f}%',\n",
    "    'Precision@5': '{:.3f}',\n",
    "    'Best Params': lambda x: x.replace(', ', ',\\n')\n",
    "}).set_properties(**{'text-align': 'left'})\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45ff182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training LightFM-WARP ===\n",
      "\n",
      "\n",
      "=== Training LightFM-BPR ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluation\n",
    "results = []\n",
    "for model_name, model_instance in lmodels.items():\n",
    "    print(f\"\\n=== Training {model_name} ===\\n\")\n",
    "    model_result = {'Model': model_name}\n",
    "    model_instance.fit(interactions, epochs=30)\n",
    "\n",
    "    # Generate valid test indices (positional)\n",
    "    valid_indices = []\n",
    "    test_user_ids = []\n",
    "    test_item_ids = []\n",
    "    for pos_idx, (idx, row) in enumerate(test_df.iterrows()):\n",
    "        uid, iid, rating = row['UserID'], row['MovieID'], row['Rating']\n",
    "        if uid in user_map and iid in item_map:  # Ensure user and item were in training\n",
    "            valid_indices.append(pos_idx)  # Store positional index\n",
    "            test_user_ids.append(user_map[uid])\n",
    "            test_item_ids.append(item_map[iid])\n",
    "\n",
    "    # Predict and scale to rating range\n",
    "    preds = model_instance.predict(test_user_ids, test_item_ids)\n",
    "    min_rating, max_rating = 1, 5\n",
    "    min_pred, max_pred = np.min(preds), np.max(preds)\n",
    "    if max_pred != min_pred:  # Avoid division by zero\n",
    "        scaled_preds = min_rating + (preds - min_pred) * (max_rating - min_rating) / (max_pred - min_pred)\n",
    "    else:\n",
    "        scaled_preds = np.full_like(preds, min_rating)  # Fallback if all predictions are the same\n",
    "\n",
    "    # Create Prediction objects using positional indices\n",
    "    predictions = [\n",
    "        Prediction(\n",
    "            uid=test_df.iloc[pos_idx]['UserID'],\n",
    "            iid=test_df.iloc[pos_idx]['MovieID'],\n",
    "            r_ui=test_df.iloc[pos_idx]['Rating'],\n",
    "            est=float(scaled_preds[j]),\n",
    "            details=None,\n",
    "        )\n",
    "        for j, pos_idx in enumerate(valid_indices)\n",
    "    ]\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = compute_metrics(predictions)\n",
    "    precision = precision_at_k(model_instance, interactions, k=5).mean()\n",
    "    model_result.update(metrics)\n",
    "    model_result.update({'Precision@5': precision})\n",
    "    results.append(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7663d00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Results ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_59c42_row0_col0, #T_59c42_row0_col1, #T_59c42_row0_col2, #T_59c42_row0_col3, #T_59c42_row1_col0, #T_59c42_row1_col1, #T_59c42_row1_col2, #T_59c42_row1_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_59c42\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_59c42_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_59c42_level0_col1\" class=\"col_heading level0 col1\" >RMSE</th>\n",
       "      <th id=\"T_59c42_level0_col2\" class=\"col_heading level0 col2\" >Acc (±0.5)</th>\n",
       "      <th id=\"T_59c42_level0_col3\" class=\"col_heading level0 col3\" >Precision@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_59c42_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_59c42_row0_col0\" class=\"data row0 col0\" >LightFM-WARP</td>\n",
       "      <td id=\"T_59c42_row0_col1\" class=\"data row0 col1\" >1.536</td>\n",
       "      <td id=\"T_59c42_row0_col2\" class=\"data row0 col2\" >26.8%</td>\n",
       "      <td id=\"T_59c42_row0_col3\" class=\"data row0 col3\" >0.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c42_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_59c42_row1_col0\" class=\"data row1 col0\" >LightFM-BPR</td>\n",
       "      <td id=\"T_59c42_row1_col1\" class=\"data row1 col1\" >1.077</td>\n",
       "      <td id=\"T_59c42_row1_col2\" class=\"data row1 col2\" >34.0%</td>\n",
       "      <td id=\"T_59c42_row1_col3\" class=\"data row1 col3\" >0.386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7276b9572080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Final Results ===\\n\")\n",
    "styled_df = results_df.style.format({\n",
    "    'RMSE': '{:.3f}',\n",
    "    'Acc (±0.5)': '{:.1f}%',\n",
    "    'Precision@5': '{:.3f}',\n",
    "    'Best Params': lambda x: x.replace(', ', ',\\n')\n",
    "}).set_properties(**{'text-align': 'left'})\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9ee960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LightFM-WARP saved successfully to ../models/lightfm_warp_model.pkl!\n",
      "Model LightFM-BPR saved successfully to ../models/lightfm_bpr_model.pkl!\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Save each LightFM model\n",
    "for model_name, model_instance in lmodels.items():\n",
    "    file_path = f'../models/{model_name.lower().replace(\"-\", \"_\")}_model.pkl'\n",
    "    dump(model_instance, file_path)\n",
    "    print(f\"Model {model_name} saved successfully to {file_path}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a9f866a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/item_map.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the mappings\n",
    "dump(user_map, '../models/user_map.pkl')\n",
    "dump(item_map, '../models/item_map.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeafef7",
   "metadata": {},
   "source": [
    "# Test Section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6307ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightFM(no_components=20, loss='bpr', learning_rate=0.005, item_alpha=0.02, user_alpha=0.02, random_state=42)\n",
    "model.fit(interactions, epochs=30, num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_user_ids, test_item_ids, num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b33e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = precision_at_k(model, interactions, k=5, num_threads=4)\n",
    "print(f\"Precision at k=5: {score.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06ee873",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([pred.est for pred in predictions])\n",
    "actuals = np.array([pred.r_ui for pred in predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2857a35c",
   "metadata": {},
   "source": [
    "# Test Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa389a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RMSE and accuracy metrics\n",
    "def compute_metrics(predictions, tolerance=0.5):\n",
    "    actuals = np.array([pred.r_ui for pred in predictions])\n",
    "    preds = np.array([pred.est for pred in predictions])\n",
    "    rmse_val = np.sqrt(np.mean((preds - actuals) ** 2))\n",
    "    accuracy = np.mean(np.abs(preds - actuals) <= tolerance) * 100\n",
    "    return {'RMSE': rmse_val, f'Acc (±{tolerance})': accuracy}\n",
    "\n",
    "# Compute precision@k for top-N recommendations\n",
    "def compute_precision_at_k(predictions, k=5, threshold=3.5):\n",
    "    user_est_true = {}\n",
    "    for pred in predictions:\n",
    "        uid, iid, true_r, est, _ = pred\n",
    "        if uid not in user_est_true:\n",
    "            user_est_true[uid] = []\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    precisions = []\n",
    "    for uid, ratings in user_est_true.items():\n",
    "        ratings.sort(key=lambda x: x[0], reverse=True)  # Sort by predicted rating\n",
    "        top_k = [r[1] >= threshold for r in ratings[:k]]  # True ratings >= threshold\n",
    "        if top_k:\n",
    "            precisions.append(sum(top_k) / len(top_k))\n",
    "    \n",
    "    return np.mean(precisions) if precisions else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc64975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7af775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = model.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "metrics = compute_metrics(predictions)\n",
    "precision_k = compute_precision_at_k(predictions, k=5, threshold=3.5)\n",
    "metrics.update({'Precision@5': precision_k, 'Best Params': str(best_params)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2049d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "results = [metrics]\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== CF Module Results (SVD) ===\\n\")\n",
    "styled_df = results_df.style.format({\n",
    "    'RMSE': '{:.3f}',\n",
    "    'Acc (±0.5)': '{:.1f}%',\n",
    "    'Precision@5': '{:.3f}',\n",
    "    'Best Params': lambda x: x.replace(', ', ',\\n')\n",
    "}).set_properties(**{'text-align': 'left'})\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8578c380",
   "metadata": {},
   "source": [
    "# Test Section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf556f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RMSE and accuracy metrics\n",
    "def compute_metrics(predictions, tolerance=0.5):\n",
    "    actuals = np.array([pred.r_ui for pred in predictions])\n",
    "    preds = np.array([pred.est for pred in predictions])\n",
    "    rmse_val = np.sqrt(np.mean((preds - actuals) ** 2))\n",
    "    accuracy = np.mean(np.abs(preds - actuals) <= tolerance) * 100\n",
    "    return {'RMSE': rmse_val, f'Acc (±{tolerance})': accuracy}\n",
    "\n",
    "# Compute precision@k\n",
    "def compute_precision_at_k(predictions, k=5, threshold=3.5):\n",
    "    user_est_true = {}\n",
    "    for pred in predictions:\n",
    "        uid, iid, true_r, est, _ = pred\n",
    "        if uid not in user_est_true:\n",
    "            user_est_true[uid] = []\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    precisions = []\n",
    "    for uid, ratings in user_est_true.items():\n",
    "        ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        top_k = [r[1] >= threshold for r in ratings[:k]]\n",
    "        if top_k:\n",
    "            precisions.append(sum(top_k) / len(top_k))\n",
    "    \n",
    "    return np.mean(precisions) if precisions else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85392b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_factors': [50, 100],\n",
    "    'n_epochs': [20, 30],\n",
    "    'lr_all': [0.005, 0.01],\n",
    "    'reg_all': [0.02, 0.1]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(SVDpp, param_grid, measures=['rmse'], cv=5, n_jobs=-1)\n",
    "gs.fit(data)\n",
    "\n",
    "# Best parameters\n",
    "best_params = gs.best_params['rmse']\n",
    "print(f\"Best RMSE: {gs.best_score['rmse']:.4f}\")\n",
    "print(f\"Best params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f985299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model\n",
    "model = SVDpp(**best_params, random_state=42)\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397867ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = model.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "metrics = compute_metrics(predictions)\n",
    "precision_k = compute_precision_at_k(predictions, k=5, threshold=3.5)\n",
    "metrics.update({'Precision@5': precision_k, 'Best Params': str(best_params)})\n",
    "\n",
    "# Display results\n",
    "results = [metrics]\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== CF Module Results (SVDpp) ===\\n\")\n",
    "styled_df = results_df.style.format({\n",
    "    'RMSE': '{:.3f}',\n",
    "    'Acc (±0.5)': '{:.1f}%',\n",
    "    'Precision@5': '{:.3f}',\n",
    "    'Best Params': lambda x: x.replace(', ', ',\\n')\n",
    "}).set_properties(**{'text-align': 'left'})\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93534d36",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35861455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and parameter grids\n",
    "models = [\n",
    "    {\n",
    "        'name': 'LightFM-WARP',\n",
    "        'algo': LightFM,\n",
    "        'params': {\n",
    "            'no_components': [20, 50],\n",
    "            'loss': ['warp'],\n",
    "            'learning_rate': [0.01, 0.03],\n",
    "            'item_alpha': [0.02, 0.1],\n",
    "            'user_alpha': [0.02, 0.1],\n",
    "            'random_state': [42]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'LightFM-BPR',\n",
    "        'algo': LightFM,\n",
    "        'params': {\n",
    "            'no_components': [20, 50],\n",
    "            'loss': ['bpr'],\n",
    "            'learning_rate': [0.01, 0.03],\n",
    "            'item_alpha': [0.02, 0.1],\n",
    "            'user_alpha': [0.02, 0.1],\n",
    "            'random_state': [42]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'SVD',\n",
    "        'algo': SVD,\n",
    "        'params': {\n",
    "            'n_factors': [50, 100, 150],\n",
    "            'n_epochs': [20, 30],\n",
    "            'lr_all': [0.005, 0.01],\n",
    "            'reg_all': [0.02, 0.1]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'KNNBasic',\n",
    "        'algo': KNNBasic,\n",
    "        'params': {\n",
    "            'k': [20, 40],\n",
    "            'sim_options': {\n",
    "                'name': ['msd', 'pearson'],\n",
    "                'user_based': [False]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'NMF',\n",
    "        'algo': NMF,\n",
    "        'params': {\n",
    "            'n_factors': [10, 15],\n",
    "            'n_epochs': [50, 100]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'CoClustering',\n",
    "        'algo': CoClustering,\n",
    "        'params': {\n",
    "            'n_cltr_u': [3, 5],\n",
    "            'n_cltr_i': [3, 5],\n",
    "            'n_epochs': [20, 30]\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c144c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surprise_to_lightfm(trainset):\n",
    "    \"\"\"Convert Surprise Trainset to COO matrix for LightFM\"\"\"\n",
    "    rows, cols, data = [], [], []\n",
    "    for uid in trainset.all_users():\n",
    "        user_ratings = trainset.ur[uid]\n",
    "        for iid, rating in user_ratings:\n",
    "            rows.append(uid)\n",
    "            cols.append(iid)\n",
    "            data.append(1)  # Use 1 for implicit feedback\n",
    "    return coo_matrix((data, (rows, cols))), trainset.n_users, trainset.n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83abda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions, _, _ = surprise_to_lightfm(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your training loop:\n",
    "for model_config in models:\n",
    "    if 'LightFM' in model_config['name']:\n",
    "        # LightFM handling\n",
    "        #interactions, _, _ = surprise_to_lightfm(trainset)\n",
    "        \n",
    "        # Hyperparameter tuning\n",
    "        best_score = -np.inf\n",
    "        best_params = {}\n",
    "        for params in ParameterGrid(model_config['params']):\n",
    "            print(f\"Training {model_config['name']} with params: {params}\")\n",
    "            model = LightFM(**params)\n",
    "            model.fit(interactions, epochs=10, verbose=False)\n",
    "            score = precision_at_k(model, interactions, k=5).mean()\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "                \n",
    "        # Final training\n",
    "        print(f\"[Final] Training {model_config['name']} with params: {params}\")\n",
    "        final_model = LightFM(**best_params)\n",
    "        final_model.fit(interactions, epochs=20)\n",
    "        \n",
    "        # Generate predictions (example for LightFM)\n",
    "        user_ids = np.arange(interactions.shape[0])\n",
    "\n",
    "    else:\n",
    "    # Original Surprise handling\n",
    "        print(f\"Training {model_config['name']} with params: {params}\")\n",
    "        gs = GridSearchCV(\n",
    "            model_config['algo'],\n",
    "            model_config['params'],\n",
    "            measures=['rmse'],\n",
    "            cv=5\n",
    "        )\n",
    "        gs.fit(data)\n",
    "        best_model = gs.best_estimator['rmse']\n",
    "        best_model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845292d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert testset to LightFM-compatible format\n",
    "test_user_ids = [trainset.to_inner_uid(uid) for (uid, _, _) in testset]\n",
    "test_item_ids = [trainset.to_inner_iid(iid) for (_, iid, _) in testset]\n",
    "\n",
    "# Generate predictions only for test pairs\n",
    "test_preds = final_model.predict(test_user_ids, test_item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = final_model.predict(user_ids, np.arange(interactions.shape[1]))\n",
    "#print(\"Train precision: %.2f\" % precision_at_k(model, interactions, k=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b4f57",
   "metadata": {},
   "source": [
    "best_params = {}\n",
    "\n",
    "for model_config in models:\n",
    "    print(f\"\\n=== Tuning {model_config['name']} ===\")\n",
    "    gs = GridSearchCV(\n",
    "        model_config['algo'],\n",
    "        model_config['params'],\n",
    "        measures=['rmse'],\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        pre_dispatch='2*n_jobs'\n",
    "    )\n",
    "    gs.fit(data)\n",
    "    best_params[model_config['name']] = gs.best_params['rmse']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f81d7b",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f2a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for model_config in models:\n",
    "    if 'LightFM' in model_config['name']:\n",
    "        continue  # Skip LightFM for now\n",
    "    print(f\"\\n=== Training {model_config['name']} ===\")\n",
    "    \n",
    "    # Initialize with best params\n",
    "    model = model_config['algo'](**best_params[model_config['name']])\n",
    "    model.fit(trainset)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = model.test(testset)\n",
    "    preds = np.array([pred.est for pred in predictions])\n",
    "    actuals = np.array([pred.r_ui for pred in predictions])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(np.mean((preds - actuals) ** 2))\n",
    "    tol_1 = np.mean(np.abs(preds - actuals) <= 1) * 100\n",
    "    tol_05 = np.mean(np.abs(preds - actuals) <= 0.5) * 100\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_config['name'],\n",
    "        'Best Params': best_params[model_config['name']],\n",
    "        'RMSE': rmse,\n",
    "        'Acc (±1)': tol_1,\n",
    "        'Acc (±0.5)': tol_05\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80316310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each model's predictions:\n",
    "tolerance = 1\n",
    "stricter_tolerance = 0.5\n",
    "\n",
    "for model_result in results:\n",
    "    model_name = model_result['Model']\n",
    "    print(f\"\\n{model_name} Accuracy:\")\n",
    "    print(f\"Within ±{tolerance} Stars: {model_result['Acc (±1)']:.2f}%\")\n",
    "    print(f\"Within ±{stricter_tolerance} Stars: {model_result['Acc (±0.5)']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06806afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds = np.array([pred.est for pred in test_preds]).reshape(-1, 1)\n",
    "#actuals = np.array([pred.r_ui for pred in test_preds]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba540de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print predictions\n",
    "#for pred in test_preds:\n",
    "#    print(f\"Predicted={pred.est:.2f}, Actual={pred.r_ui}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f51e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Optional: Formatting for better display\n",
    "results_df.style.format({\n",
    "    'RMSE': '{:.4f}',\n",
    "    'Acc (±1)': '{:.2f}%',\n",
    "    'Acc (±0.5)': '{:.2f}%'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tolerance (e.g., predictions within ±1 stars are \"correct\")\n",
    "tolerance = 1\n",
    "correct = np.abs(preds - actuals) <= tolerance\n",
    "test_accuracy = np.mean(correct) * 100\n",
    "\n",
    "# Define stricter tolerance (e.g., predictions within ±0.5 stars are \"correct\")\n",
    "stricter_tolerance = 0.5\n",
    "s_correct = np.abs(preds - actuals) <= stricter_tolerance\n",
    "s_test_accuracy = np.mean(s_correct) * 100\n",
    "\n",
    "print(f\"Accuracy (Within ±{tolerance} Stars): {test_accuracy:.2f}%\")\n",
    "print(f\"Accuracy (Within ±{stricter_tolerance} Stars): {s_test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0612cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "dump(model, '../models/cf_model.pkl')  # Or use .joblib extension\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa8235",
   "metadata": {},
   "source": [
    "# Model Training with 10M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Upgrade\n",
    "beeg_data = pd.read_csv(r\"K:\\MachineProject\\Data\\ml-32m\\ratings.dat\", sep='::', engine='python', names=['UserID', 'MovieID', 'Rating', 'Timestamp'])\n",
    "\n",
    "# Drop the Timestamp column\n",
    "beeg_data = beeg_data.drop('Timestamp', axis=1)\n",
    "beeg_data.columns = ['UserID', 'MovieID', 'Rating']\n",
    "beeg_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd291d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_per_user = beeg_data.groupby('UserID')['Rating'].count().reset_index()\n",
    "ratings_per_user.columns = ['user_id', 'num_ratings']\n",
    "print(ratings_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4844132",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ratings_per_user['num_ratings'].describe(percentiles=[0.1, 0.5, 0.9])\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d9cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "beeg_data[\"Rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "beeg_data[\"MovieID\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d651be",
   "metadata": {},
   "outputs": [],
   "source": [
    "beeg_data[\"UserID\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c1c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "beeg_data['UserID'] = beeg_data['UserID'].astype('int32')\n",
    "beeg_data['MovieID'] = beeg_data['MovieID'].astype('int32')\n",
    "beeg_data['Rating'] = beeg_data['Rating'].astype('float16')\n",
    "\n",
    "beeg_data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify by user_id (ensure all users are represented)\n",
    "subsampled_df, _ = sklearn_split(\n",
    "    beeg_data,\n",
    "    test_size=0.5,\n",
    "    stratify=beeg_data['UserID'],  # Preserve user distribution\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab49a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(beeg_data['Rating'].min(), beeg_data['Rating'].max()))\n",
    "data = Dataset.load_from_df(beeg_data[['UserID', 'MovieID', 'Rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(location='./cache', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8a3a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_factors': [50, 100],  # Test latent dimensions\n",
    "    'n_epochs': [20, 30],\n",
    "    'lr_all': [0.005, 0.01],\n",
    "    'reg_all': [0.02, 0.1]\n",
    "}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    SVD,\n",
    "    param_grid,\n",
    "    measures=['rmse'],\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    ")\n",
    "gs.fit(data)\n",
    "\n",
    "# Best RMSE score and params\n",
    "print(f\"Best RMSE: {gs.best_score['rmse']}\")\n",
    "print(f\"Best params: {gs.best_params['rmse']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876697af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09545c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVD(n_factors=50, n_epochs=20, lr_all=0.005, reg_all=0.02, random_state=42)\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b27573",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.test(testset)\n",
    "accuracy.rmse(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e7b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([pred.est for pred in test_preds]).reshape(-1, 1)\n",
    "actuals = np.array([pred.r_ui for pred in test_preds]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d256c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tolerance (e.g., predictions within ±1 stars are \"correct\")\n",
    "tolerance = 1\n",
    "correct = np.abs(preds - actuals) <= tolerance\n",
    "test_accuracy = np.mean(correct) * 100\n",
    "\n",
    "# Define stricter tolerance (e.g., predictions within ±0.5 stars are \"correct\")\n",
    "stricter_tolerance = 0.5\n",
    "s_correct = np.abs(preds - actuals) <= stricter_tolerance\n",
    "s_test_accuracy = np.mean(s_correct) * 100\n",
    "\n",
    "print(f\"Accuracy (Within ±{tolerance} Stars): {test_accuracy:.2f}%\")\n",
    "print(f\"Accuracy (Within ±{stricter_tolerance} Stars): {s_test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ad150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "dump(model, '../models/cf_model_2.pkl')  # Or use .joblib extension\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab4e897",
   "metadata": {},
   "source": [
    "# Model Training with 32M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645626c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Upgrade\n",
    "beegar_data = pd.read_csv(r\"K:\\MachineProject\\Data\\ml-32m\\ratings.csv\")\n",
    "\n",
    "# Drop the Timestamp column\n",
    "beegar_data = beegar_data.drop('timestamp', axis=1)\n",
    "beegar_data.columns = ['UserID', 'MovieID', 'Rating']\n",
    "beegar_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_per_user = beegar_data.groupby('UserID')['Rating'].count().reset_index()\n",
    "ratings_per_user.columns = ['user_id', 'num_ratings']\n",
    "print(ratings_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118825b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ratings_per_user['num_ratings'].describe(percentiles=[0.1, 0.5, 0.9])\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee1c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "beegar_data[\"Rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "beegar_data[\"MovieID\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82463015",
   "metadata": {},
   "outputs": [],
   "source": [
    "beegar_data[\"UserID\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "beegar_data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "beegar_data['UserID'] = beegar_data['UserID'].astype('int32')\n",
    "beegar_data['MovieID'] = beegar_data['MovieID'].astype('int32')\n",
    "beegar_data['Rating'] = beegar_data['Rating'].astype('float16')\n",
    "\n",
    "beegar_data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46ee50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify by user_id (ensure all users are represented)\n",
    "subsampled_df, _ = sklearn_split(\n",
    "    beegar_data,\n",
    "    test_size=0.5,\n",
    "    stratify=beegar_data['UserID'],  # Preserve user distribution\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f50baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(beegar_data['Rating'].min(), beegar_data['Rating'].max()))\n",
    "data = Dataset.load_from_df(beegar_data[['UserID', 'MovieID', 'Rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93440a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(location='./cache', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8851ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_factors': [50, 100],  # Test latent dimensions\n",
    "    'n_epochs': [20, 30],\n",
    "    'lr_all': [0.005, 0.01],\n",
    "    'reg_all': [0.02, 0.1]\n",
    "}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    SVD,\n",
    "    param_grid,\n",
    "    measures=['rmse'],\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    ")\n",
    "gs.fit(data)\n",
    "\n",
    "# Best RMSE score and params\n",
    "print(f\"Best RMSE: {gs.best_score['rmse']}\")\n",
    "print(f\"Best params: {gs.best_params['rmse']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5fffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71fabf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVD(n_factors=50, n_epochs=20, lr_all=0.005, reg_all=0.02, random_state=42)\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e87ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.test(testset)\n",
    "accuracy.rmse(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce5511",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([pred.est for pred in test_preds]).reshape(-1, 1)\n",
    "actuals = np.array([pred.r_ui for pred in test_preds]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e7299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tolerance (e.g., predictions within ±1 stars are \"correct\")\n",
    "tolerance = 1\n",
    "correct = np.abs(preds - actuals) <= tolerance\n",
    "test_accuracy = np.mean(correct) * 100\n",
    "\n",
    "# Define stricter tolerance (e.g., predictions within ±0.5 stars are \"correct\")\n",
    "stricter_tolerance = 0.5\n",
    "s_correct = np.abs(preds - actuals) <= stricter_tolerance\n",
    "s_test_accuracy = np.mean(s_correct) * 100\n",
    "\n",
    "print(f\"Accuracy (Within ±{tolerance} Stars): {test_accuracy:.2f}%\")\n",
    "print(f\"Accuracy (Within ±{stricter_tolerance} Stars): {s_test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "dump(model, '../models/cf_model_2.pkl')  # Or use .joblib extension\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLab10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
